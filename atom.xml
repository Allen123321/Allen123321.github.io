<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="https://allen123321.github.io/atom.xml" rel="self"/>
  
  <link href="https://allen123321.github.io/"/>
  <updated>2021-08-12T05:06:11.343Z</updated>
  <id>https://allen123321.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>西瓜书第4章</title>
    <link href="https://allen123321.github.io/2021/08/12/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC4%E7%AB%A0/"/>
    <id>https://allen123321.github.io/2021/08/12/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC4%E7%AB%A0/</id>
    <published>2021-08-12T03:05:28.000Z</published>
    <updated>2021-08-12T05:06:11.343Z</updated>
    
    <content type="html"><![CDATA[<h1 id="西瓜书第4章-决策树"><a href="#西瓜书第4章-决策树" class="headerlink" title="西瓜书第4章 决策树"></a>西瓜书第4章 决策树</h1><h2 id="1-算法原理"><a href="#1-算法原理" class="headerlink" title="1. 算法原理"></a>1. 算法原理</h2><p>决策树是基于树结构对问题进行决策或判定的过程。</p><p>决策过程中提出的判定问题（内部节点）是对某个属性的“测试”，每个测试的结果可以导出最终结论（叶节点）或导出进一步判定问题（下一层内部节点，其考虑范围是在上次决策结果的限定范围之内）。</p><p>算法核心：选取划分条件（划分属性）。</p><p>最终目的：样本划分越“纯”越好。</p><p>在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：</p><hr><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">女儿：多大年纪了？</span><br><span class="line">母亲：26。</span><br><span class="line">女儿：长的帅不帅？</span><br><span class="line">母亲：挺帅的。</span><br><span class="line">女儿：收入高不？</span><br><span class="line">母亲：不算很高，中等情况。</span><br><span class="line">女儿：是公务员不？</span><br><span class="line">母亲：是，在税务局上班呢。</span><br><span class="line">女儿：那好，我去见见。</span><br></pre></td></tr></table></figure><hr><p>这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。</p><hr><p><img src="https://camo.githubusercontent.com/af42998129b02563a6687be4be683621e9b8111cc2fbe1bf0ae53a40417ca6d2/68747470733a2f2f692e6c6f6c692e6e65742f323031382f31302f31372f356263373238656338346137372e706e67" alt="1.png"></p><hr><p>在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：</p><pre><code>* 每个非叶节点表示一个特征属性测试。* 每个分支代表这个特征属性在某个值域上的输出。* 每个叶子节点存放一个类别。* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。</code></pre><h2 id="2-决策树的构造"><a href="#2-决策树的构造" class="headerlink" title="2. 决策树的构造"></a>2. 决策树的构造</h2><p>决策树的构造是一个递归的过程，有三种情形会导致递归返回：</p><ol><li>当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；</li><li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；</li><li>当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。</li></ol><p><strong>算法基本流程伪码</strong></p><hr><img src="/2021/08/12/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC4%E7%AB%A0/algorithm.png" class="" title="This is df algorithm"><hr><p>可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。</p><h2 id="3-ID3算法"><a href="#3-ID3算法" class="headerlink" title="3. ID3算法"></a>3. ID3算法</h2><p>ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：</p><script type="math/tex; mode=display">Ent(D) = - \sum_{k=1}^{|y|} P_{k} log_{2} P_{k}</script><p><strong>信息熵值越大越混乱。</strong></p><p>假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。</p><script type="math/tex; mode=display">Gain(D,a) = Ent(D) - \sum_{v=1}^V \frac{|D^v|}{|D|} Ent(D^v)</script><p>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。</p><h2 id="4-C4-5算法"><a href="#4-C4-5算法" class="headerlink" title="4. C4.5算法"></a>4. C4.5算法</h2><p>ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：</p><script type="math/tex; mode=display">Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}</script><p>其中：</p><script type="math/tex; mode=display">IV(a) = - \sum_{v = 1}^{V} \frac{|D^v|}{|D|} log_{2}\frac{|D^v|}{|D|}</script><p>IV(a) 称为属性a的”固有值”。属性a的可能取值数目越多(即越大)，则IV(a)的值通常会越大。</p><p>C4.5 算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：<br>先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p><h2 id="5-CART算法"><a href="#5-CART算法" class="headerlink" title="5. CART算法"></a>5. CART算法</h2><p>CART是Classification and Regression Tree的简称，这是一种著名的决策树学习算法，分类和回归任务都可用。其生成的决策树为二叉树。</p><p>CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，基尼指数定义如下：</p><script type="math/tex; mode=display">Gini(D) = \sum_{k=1}^{|y|} \sum_{k^{'} \neq k} P_{k}P{k^{'}}  \\ = 1- \sum_{k=1}^{|y|} P_{k}^2</script><p>任取两个样本不一致的概率，越小表示集合越纯</p><p>进而，使用属性α划分后的基尼指数为：</p><script type="math/tex; mode=display">Gini\_index(D,a) = \sum_{v=1}^{V} \frac{|D^v|}{|D|} Gini(D^v)</script><p>选择基尼指数最小的属性作为最优划分属性：</p><script type="math/tex; mode=display">a_{*} = \mathop{\arg\min}_{a \in A} Gini\_index(D,a)</script><p><strong>算法流程</strong></p><ol><li>对每个属性a的每个可能取值v，将数据集D分为$a=v$和​​$a \neq v $两部分来计算基尼指数，即：</li></ol><script type="math/tex; mode=display">Gini\_index(D,a) = \frac{|D^{a=v}|}{|D|} Gini(D^{a = v}) + \frac{|D^{a \neq v}|}{|D|} Gini(D^{a \neq v})</script><ol><li><p>选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；</p></li><li><p>重复以上两步，直至满足停止条件。</p></li></ol><h2 id="6-剪枝处理"><a href="#6-剪枝处理" class="headerlink" title="6. 剪枝处理"></a>6. 剪枝处理</h2><p>剪枝(pruning)是决策树学习算法对付”过拟合”的主要手段。，为了尽可能正确分类训练样本，有时会造成决策树分支过多，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。可通过主动去掉一些分支来降低过拟合的风险。</p><p>剪枝基本策略包括”<strong>预剪枝</strong>“(prepruning)和”<strong>后剪枝</strong>“(post”pruning)。</p><p>可以用交叉验证的方法，即预留一部分数据用作”验证集”以进行性 能评估，以判断剪枝是否能带来算法泛化性能的提升。</p><p><strong>预剪枝</strong></p><p>预剪枝指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。</p><ul><li><p>优点<br>预剪枝使得决策树的很多分支都没有”展开“，可以降低过拟合的风险；<br>显著减少决策树的训练时间开销和测试时间开销。</p></li><li><p>缺点<br>有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显 著提高；</p></li></ul><p>预剪枝基于”贪心”本质禁止这些分支展开，可能导致欠拟合。</p><p><strong>后剪枝</strong></p><p>后剪枝先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。</p><p>根据奥卡姆剃刀准则，剪枝后的模型更好。因此，后剪枝下，决策树算法在验证集精度虽无提高的情况中会进行剪枝。</p><ul><li>优点<br>一般情形下，后剪枝决策树的欠拟合风险很小，泛化能往往优于预剪枝决策树；</li><li>缺点<br>后剪枝过程是在生成完全决策树之后进行的，并且要白底向上对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。</li></ul><h2 id="7-连续值与缺失值处理"><a href="#7-连续值与缺失值处理" class="headerlink" title="7. 连续值与缺失值处理"></a>7. 连续值与缺失值处理</h2><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。</p><ul><li>首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。</li><li>计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。</li><li>选择最大信息增益的划分点作为最优划分点。<script type="math/tex; mode=display">Gain(D,a) = \mathop{\max}_{t \in T_{a}} Gain(D,a,t)  \\  = \mathop{\max}_{t \in T_{a}} Ent(D) - \sum_{\lambda \in {-,+}} \frac{|D_{t}^{\lambda}|}{|D|} Ent(D_{t}^{\lambda})</script></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;西瓜书第4章-决策树&quot;&gt;&lt;a href=&quot;#西瓜书第4章-决策树&quot; class=&quot;headerlink&quot; title=&quot;西瓜书第4章 决策树&quot;&gt;&lt;/a&gt;西瓜书第4章 决策树&lt;/h1&gt;&lt;h2 id=&quot;1-算法原理&quot;&gt;&lt;a href=&quot;#1-算法原理&quot; class=&quot;</summary>
      
    
    
    
    <category term="机器学习" scheme="https://allen123321.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="西瓜书" scheme="https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书第3章</title>
    <link href="https://allen123321.github.io/2021/08/06/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC3%E7%AB%A0/"/>
    <id>https://allen123321.github.io/2021/08/06/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC3%E7%AB%A0/</id>
    <published>2021-08-06T07:51:47.000Z</published>
    <updated>2021-08-07T03:31:17.406Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第三章-线性模型"><a href="#第三章-线性模型" class="headerlink" title="第三章 线性模型"></a>第三章 线性模型</h1><h2 id="3、线性模型"><a href="#3、线性模型" class="headerlink" title="3、线性模型"></a><strong>3、线性模型</strong></h2><p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p><h3 id="3-1-线性回归"><a href="#3-1-线性回归" class="headerlink" title="3.1 线性回归"></a><strong>3.1 线性回归</strong></h3><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000—&gt;13亿…2016—&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）—&gt;15k。</p><p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p><ul><li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p></li><li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p></li></ul><p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance 欧氏距离），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p><p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成：</p><p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p><p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p><p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p><p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p><p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p><h3 id="3-2-线性几率回归"><a href="#3-2-线性几率回归" class="headerlink" title="3.2 线性几率回归"></a><strong>3.2 线性几率回归</strong></h3><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p><p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面列出求解的思路。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p><h3 id="3-3-线性判别分析"><a href="#3-3-线性判别分析" class="headerlink" title="3.3 线性判别分析"></a><strong>3.3 线性判别分析</strong></h3><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p><p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p><ul><li>类内散度矩阵（within-class scatter matrix）</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p><ul><li>类间散度矩阵(between-class scaltter matrix)</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p><p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p><p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p><p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。<br>​             </p><h3 id="3-4-多分类学习"><a href="#3-4-多分类学习" class="headerlink" title="3.4 多分类学习"></a><strong>3.4 多分类学习</strong></h3><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p><ul><li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p></li><li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p></li><li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。</p></li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p><h3 id="3-5-类别不平衡问题"><a href="#3-5-类别不平衡问题" class="headerlink" title="3.5 类别不平衡问题"></a><strong>3.5 类别不平衡问题</strong></h3><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p><ol><li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li><li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li><li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li></ol><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><ol><li><a href="https://github.com/Allen123321/DEMO-DL/blob/master/LogisticRegression/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E4%BE%8B.ipynb">线性回归实例python代码实现</a></li><li><a href="https://github.com/Allen123321/DEMO-DL/blob/master/LogisticRegression/Iris_flower_data_classification.ipynb">Iris数据集线性分类sklearn</a></li><li><a href="https://github.com/Allen123321/DEMO-DL/blob/master/LogisticRegression/boston_housing.ipynb">boston housing 数据集线性回归实例</a></li><li><a href="https://github.com/Allen123321/DEMO-DL/blob/master/LogisticRegression/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.ipynb">线性回归实例</a></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>《机器学习》——周志华</p><p>【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集</p><p>  <a href="https://github.com/datawhalechina/machine-learning-toy-code">Datawhale</a><br>  <a href="https://github.com/Vay-keen/Machine-learning-learning-notes">周志华《机器学习》的学习笔记</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第三章-线性模型&quot;&gt;&lt;a href=&quot;#第三章-线性模型&quot; class=&quot;headerlink&quot; title=&quot;第三章 线性模型&quot;&gt;&lt;/a&gt;第三章 线性模型&lt;/h1&gt;&lt;h2 id=&quot;3、线性模型&quot;&gt;&lt;a href=&quot;#3、线性模型&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="Machine learning" scheme="https://allen123321.github.io/categories/Machine-learning/"/>
    
    
    <category term="西瓜书" scheme="https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书第2章</title>
    <link href="https://allen123321.github.io/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/"/>
    <id>https://allen123321.github.io/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/</id>
    <published>2021-08-05T02:04:47.000Z</published>
    <updated>2021-08-05T06:32:06.969Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第二章学习笔记"><a href="#第二章学习笔记" class="headerlink" title="第二章学习笔记"></a>第二章学习笔记</h1><h2 id="模型的评估与选择"><a href="#模型的评估与选择" class="headerlink" title="模型的评估与选择"></a>模型的评估与选择</h2><h3 id="2-1-误差与过拟合"><a href="#2-1-误差与过拟合" class="headerlink" title="2.1 误差与过拟合"></a>2.1 误差与过拟合</h3><p>我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：</p><ul><li>在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。</li><li>在测试集上的误差称为测试误差（test error）。</li><li>学习器在所有新样本上的误差称为泛化误差（generalization error）。</li></ul><p>我们实际希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：</p><ul><li><p>学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。</p><ul><li>学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。</li></ul><p>在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/overfiting.png" class="" title="This is the overfiting-underfitting img"></li></ul><h3 id="2-2-评估方法"><a href="#2-2-评估方法" class="headerlink" title="2.2 评估方法"></a>2.2 评估方法</h3><p>在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。</p><p>因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：</p><p>假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。</p><h4 id="2-2-1-训练集与测试集的划分方法"><a href="#2-2-1-训练集与测试集的划分方法" class="headerlink" title="2.2.1 训练集与测试集的划分方法"></a>2.2.1 训练集与测试集的划分方法</h4><p>如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：</p><p><strong>留出法 （Hold-out）</strong></p><p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样（stratified sampling）保持S与T内的样本各个类别的比例和原分布相似。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p><p><em>案例代码: 利用sklearn的train_test_split函数进行数据集划分(基于留出法的原理)</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;载入数据&#x27;&#x27;&#x27;</span></span><br><span class="line">X,y = datasets.load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;采取分层抽样时的数据集分割&#x27;&#x27;&#x27;</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=<span class="number">0.3</span>,stratify=y)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;打印各个数据集的形状&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(X_train.shape,X_test.shape,y_train.shape,y_test.shape)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;打印训练集中因变量的各类别数目情况&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(pd.value_counts(y_train))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;打印验证集集中因变量的各类别数目情况&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(pd.value_counts(y_test))</span><br><span class="line"></span><br></pre></td></tr></table></figure><br><strong>交叉验证法</strong><br>将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪…∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：<strong>每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集</strong>，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/k-flod.png" class="" title="This is the 10-flod img"><p><em>利用sklearn的KFold函数划分数据集：</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">kf = KFold(n_splits=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train,test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">    <span class="built_in">print</span>(train,<span class="string">&#x27;\n&#x27;</span>,test)</span><br><span class="line">    </span><br></pre></td></tr></table></figure><br>与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。</p><p><em>利用sklearn的LeaveOneOut函数划分数据集：</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> LeaveOneOut</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">kf = LeaveOneOut()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train,test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">    <span class="built_in">print</span>(train,<span class="string">&#x27;\n&#x27;</span>,test)</span><br></pre></td></tr></table></figure></p><p><strong>自助法</strong></p><p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。</p><p>自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为：</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/gongshi.png" class="" title="This is the gongshi img"><p>这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p><p><em>自助法-数据集划分</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机产生我们的数据集</span></span><br><span class="line">x = np.random.randint(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>)  <span class="comment"># 前两个参数表示范围，第三个参数表示个数</span></span><br><span class="line">index = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x))]   <span class="comment"># 数据集的下标</span></span><br><span class="line">train_set = []  <span class="comment"># 训练集</span></span><br><span class="line">train_index = []  <span class="comment"># 用于记录训练集各个元素的下标</span></span><br><span class="line">test_set = []  <span class="comment"># 测试集</span></span><br><span class="line">test_index = []  <span class="comment"># 用于记录测试集各个元素的下标</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行m次放回抽样</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    train_index.append(np.floor(np.random.random()*<span class="built_in">len</span>(x)))</span><br><span class="line"><span class="comment"># 计算D\D&#x27;</span></span><br><span class="line">test_index = <span class="built_in">list</span>(<span class="built_in">set</span>(index).difference(<span class="built_in">set</span>(train_index)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取数，产生训练/测试集</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(train_index)):</span><br><span class="line">    train_set.append(x[<span class="built_in">int</span>(train_index[i])])  <span class="comment"># 这里记得强制转换为int型，否则会报错</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_index)):</span><br><span class="line">    test_set.append(x[<span class="built_in">int</span>(test_index[i])])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果进行验证</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;data set: &quot;</span>, x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train set: &quot;</span>, train_set)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;test set: &quot;</span>, test_set)</span><br></pre></td></tr></table></figure></p><p><strong>顺序约束采样</strong></p><p>对于具有强顺序约束的序列（比如时间序列）的数据类型，前后相邻的数据关联程度很高，在数据分割时不能打乱顺序随机采样，即不能破坏序列的连续性。</p><p><em>sklearn中提供了TimeSeriesSplit函数用于分割这种类型的数据</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> TimeSeriesSplit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">kf = TimeSeriesSplit(n_splits=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train,test <span class="keyword">in</span> kf.split(X):</span><br><span class="line">    <span class="built_in">print</span>(train,<span class="string">&#x27;\n&#x27;</span>,test)</span><br></pre></td></tr></table></figure></p><h4 id="2-2-2-调参"><a href="#2-2-2-调参" class="headerlink" title="2.2.2 调参"></a>2.2.2 调参</h4><p>常用的方法包括网格法，利用一定的步长在参数的值域范围内探索参数最优解，在开始的时候可以选择较大的步长，然后逐步缩小步长逼近最优解。</p><p>将数据集划分为三个部分：训练集、验证集、测试集。将训练集数据用于模型训练，验证集（开发集）数据用于模型调参，测试集数据用于验证模型泛化性。</p><p>给定包含个样本的数据集，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练、模型。因此，在模型选择完成后，学习算法和参数配置己选定，此时应该用数据集重新训练模型。这个模型在训练过程中使用了所有个样本，这才是我们最终提交给用户的模型。</p><h3 id="2-3-性能度量"><a href="#2-3-性能度量" class="headerlink" title="2.3 性能度量"></a>2.3 性能度量</h3><p>性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。</p><p><strong>均方误差mean squared error MSE</strong></p><p>在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数。</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/msq.png" class="" title="This is the msq img"><p><strong>错误率error rate和精度accuracy</strong></p><p>在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/accanderr.png" class="" title="This is the acc img"><p><strong>查准率（准确率precision）、查全率（召回率recall）、F1</strong></p><p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/f1.png" class="" title="This is the f1 img"><p>FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negtive，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边：</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/fnfp.png" class="" title="This is the fnfp img"><p>正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；<br>如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p><p>“P-R曲线”正是描述查准/查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/prrate.png" class="" title="This is the pr img"><p>P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。<br>但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。</p><p>P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/jiaquanpingjun.png" class="" title="This is the jiaquan img"><p>特别地，当β=1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/f11.png" class="" title="This is the f11 img"><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/f12.png" class="" title="This is the f12 img"><p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。<br>简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/microf1.png" class="" title="This is the micro img"><p><strong>ROC与AUC</strong></p><p>学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值(threshold)，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。</p><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/tprfpr.png" class="" title="This is the tpr img"><img src="/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/rocauc.png" class="" title="This is the roc img"><p>简单分析图像，可以得知：当FN=0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。</p><p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。</p><p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。<br>ROC曲线下的面积定义为AUC（Area Uder ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。<br>易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e2c57.png" alt="15.png"></p><p><strong>2.5.4 代价敏感错误率与代价曲线</strong></p><p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病—&gt;有疾病只是增多了检查，但有疾病—&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6ed582.png" alt="16.png"></p><p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率为：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed70bebe.png" alt="17.png"></p><p>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e952e.png" alt="18.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6eee7b.png" alt="19.png"></p><p>代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed716e0d.png" alt="20.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第二章学习笔记&quot;&gt;&lt;a href=&quot;#第二章学习笔记&quot; class=&quot;headerlink&quot; title=&quot;第二章学习笔记&quot;&gt;&lt;/a&gt;第二章学习笔记&lt;/h1&gt;&lt;h2 id=&quot;模型的评估与选择&quot;&gt;&lt;a href=&quot;#模型的评估与选择&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="Machine learning" scheme="https://allen123321.github.io/categories/Machine-learning/"/>
    
    
    <category term="西瓜书" scheme="https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书第1章</title>
    <link href="https://allen123321.github.io/2021/08/04/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC1%E7%AB%A0/"/>
    <id>https://allen123321.github.io/2021/08/04/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC1%E7%AB%A0/</id>
    <published>2021-08-04T00:04:25.000Z</published>
    <updated>2021-08-04T02:07:52.521Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一章学习笔记"><a href="#第一章学习笔记" class="headerlink" title="第一章学习笔记"></a>第一章学习笔记</h1><h2 id="1、引言"><a href="#1、引言" class="headerlink" title="1、引言"></a>1、引言</h2><p>傍晚小街路面上沁出微雨后的湿润，和熙的细风吹来，抬头看看天边的晚霞，嗯，明天又是一个好天气。走到水果摊旁，挑了个根蒂蜷缩、敲起来声音浊响的青绿西瓜，一边满心期待着皮薄肉厚瓢甜的爽落感，一边愉快地想着，这学期狠下了工夫，基础概念弄得清清楚楚，算法作业也是信手拈来，这门课成绩一定差不了！</p><h3 id="1-1、机器学习的定义"><a href="#1-1、机器学习的定义" class="headerlink" title="1.1、机器学习的定义"></a>1.1、机器学习的定义</h3><p>正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。</p><p>另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：</p><ul><li>P：计算机程序在某任务类T上的性能。</li><li>T：计算机程序希望实现的任务类。</li><li>E：表示经验，即历史的数据集。</li></ul><p>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。</p><h3 id="1-2、机器学习的一些基本术语"><a href="#1-2、机器学习的一些基本术语" class="headerlink" title="1.2、机器学习的一些基本术语"></a>1.2、机器学习的一些基本术语</h3><p>假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)，(色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义：</p><ul><li>所有记录的集合为：数据集。</li><li>每一条记录为：一个实例（instance）或样本（sample）。</li><li>例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。</li><li>对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。</li><li>一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。</li></ul><p>计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：</p><ul><li><p>所有训练样本的集合为：训练集（trainning set），[特殊]。</p><ul><li>所有测试样本的集合为：测试集（test set），[一般]。  </li><li>机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。</li></ul><p>西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：    </p><ul><li>预测值为离散值的问题为：分类（classification）。</li><li>预测值为连续值的问题为：回归（regression）。</li></ul><p>我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：    </p><ul><li>训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。</li><li><p>训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类（clustering）和关联规则。</p><h4 id="训练training、验证validation-develop、测试testing-的作用"><a href="#训练training、验证validation-develop、测试testing-的作用" class="headerlink" title="训练training、验证validation/develop、测试testing 的作用"></a>训练training、验证validation/develop、测试testing 的作用</h4><p>学得模型后，使用其进行预测的过程称为测试。</p><p><strong>为什么要将数据集划分为三个部分？三个部分的作用？三个部分数据集的比例应如何设定？</strong></p><p>另外一种常见的数据集划分方法是将数据集划分为两个部分（训练集和测试集），这种划分方法存在的问题在于，模型利用训练集数据进行训练，测试集数据进行模型泛化性测试。但我们不能利用测试集测试的bad case或者根据测试集的测试精度调整模型的参数。这是因为对于模型来说，数据集应该是只有训练集可见的，其他数据均不可见，如果利用测试集的结果调整模型相对于模型也”看到了“测试集的数据。将数据集划分为是独立同分布的三个部分就可以解决这个问题，将训练集数据用于模型训练，验证集（开发集）数据用于模型调参，测试集数据用于验证模型泛化性。</p><h3 id="1-3-假设空间"><a href="#1-3-假设空间" class="headerlink" title="1.3 假设空间"></a>1.3 假设空间</h3><p><strong>归纳induction和演绎deduction</strong> </p></li></ul><ul><li>归纳：从特殊到一般的“泛化”过程，从具体事实归结一般规律。</li><li><p>演绎：从一般到特殊的“特化”过程，从基础原理推演具体情况。</p><p><strong>假设hypothesis space、版本空间hypothesis space</strong></p></li><li><p>假设空间：监督学习（supervised learning）的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出 一个好的预测。模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间(hypothesis space)。我们也可以将学习过程看作一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集”匹配”的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确定，假设空间及其规模大小就确定了。</p></li><li><p>版本空间：与训练集一致的“假设集合”。</p><p><strong><a href="https://blog.csdn.net/anqijiayou/article/details/79697900">理解《机器学习》中的假设空间和版本空间</a></strong></p><h2 id="1-4、归纳偏好inductive-bias"><a href="#1-4、归纳偏好inductive-bias" class="headerlink" title="1.4、归纳偏好inductive bias"></a>1.4、归纳偏好inductive bias</h2><p>在学习过程中对某种类型假设的偏好。有效的机器学习算法必有其归纳偏好。基于某种领域的知识而产生的归纳偏好，与特征选择不同，特征选择是基于训练样本分析选择有效的特征或者更加重视某类特征。</p><p>归纳偏好与问题本身匹配，偏好什么能让模型更好。 “奥卡姆剃刀”(Occam’s razor)是一种常用的、自然科学研究中最基本的原则，用于确立”正确的”偏好——即”若有多个假设与观察一致，则选最简单的那个“。</p><p><strong>没有免费午餐定理No Free Lunch Theorem -NFL</strong><br>根据奥卡姆剃刀原则，下图(a)中A曲线的泛化能力要比曲线B强，但可能存在图(b)这样的情况，B曲线更好地满足测试集样本。也就是说，针对某种分布的样本空间（某种问题），算法A可能优于B，而针对另外一种分布的样本空间（另外一种问题），算法B可能优于A，这就引出了没有免费午餐定理。</p><img src="/2021/08/04/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC1%E7%AB%A0/NFL.png" class="" title="This is No Free Lunch Theorem img"><p>没有免费午餐定理表明：若考虑所有潜在的问题，所有学习算法的误差期望相同，也就是所有算法都一样好。因此，脱离具体问题空谈什么学习算法更好没有意义，在某些问题上表现好的学习算法在另外一些问题上可能不尽人意。学习算法自身的归纳偏好与目标问题是否匹配往往起决定作用。</p></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第一章学习笔记&quot;&gt;&lt;a href=&quot;#第一章学习笔记&quot; class=&quot;headerlink&quot; title=&quot;第一章学习笔记&quot;&gt;&lt;/a&gt;第一章学习笔记&lt;/h1&gt;&lt;h2 id=&quot;1、引言&quot;&gt;&lt;a href=&quot;#1、引言&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="Machine learning" scheme="https://allen123321.github.io/categories/Machine-learning/"/>
    
    
    <category term="西瓜书" scheme="https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>西瓜书第6章</title>
    <link href="https://allen123321.github.io/2021/07/29/ML-xgs-svm/"/>
    <id>https://allen123321.github.io/2021/07/29/ML-xgs-svm/</id>
    <published>2021-07-29T02:06:02.000Z</published>
    <updated>2021-08-03T23:58:30.373Z</updated>
    
    <content type="html"><![CDATA[<h1 id="6、支持向量机"><a href="#6、支持向量机" class="headerlink" title="6、支持向量机"></a><strong>6、支持向量机</strong></h1><p>支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。</p><h2 id="6-1-函数间隔与几何间隔"><a href="#6-1-函数间隔与几何间隔" class="headerlink" title="6.1 函数间隔与几何间隔"></a><strong>6.1 函数间隔与几何间隔</strong></h2><p>对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a2ec8a.png" alt="1.png"></p><p>对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a06d5a.png" alt="2.png"></p><h3 id="6-1-1-函数间隔"><a href="#6-1-1-函数间隔" class="headerlink" title="6.1.1 函数间隔"></a><strong>6.1.1 函数间隔</strong></h3><p>在超平面w’x+b=0确定的情况下，|w’x<em>+b|能够代表点x</em>距离超平面的远近，易知：当w’x<em>+b&gt;0时，表示x</em>在超平面的一侧（正类，类标为1），而当w’x<em>+b&lt;0时，则表示x</em>在超平面的另外一侧（负类，类别为-1），因此（w’x<em>+b）y</em> 的正负性恰能表示数据点x<em>是否被分类正确。于是便引出了<em>*函数间隔</em></em>的定义（functional margin）:</p><p><img src="https://i.loli.net/2018/10/17/5bc72f690a14b.png" alt="3.png"></p><p>而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f690ac26.png" alt="4.png"></p><p>可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念—几何间隔（geometrical margin）。</p><h3 id="6-1-2-几何间隔"><a href="#6-1-2-几何间隔" class="headerlink" title="6.1.2 几何间隔"></a><strong>6.1.2 几何间隔</strong></h3><p><strong>几何间隔</strong>代表的则是数据点到超平面的真实距离，对于超平面w’x+b=0，w代表的是该超平面的法向量，设x<em>为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x</em>=x-r(w/||w||)，又x<em>在超平面上，即w’x</em>+b=0，代入即可得：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f697d499.png" alt="5.png"></p><p>为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f696fd10.png" alt="6.png"></p><p>从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。</p><h2 id="6-2-最大间隔与支持向量"><a href="#6-2-最大间隔与支持向量" class="headerlink" title="6.2 最大间隔与支持向量"></a><strong>6.2 最大间隔与支持向量</strong></h2><p>通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f69af163.png" alt="7.png"></p><p>一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f697bb1d.png" alt="8.png"></p><p>对于y(w’x+b)=1的数据点，即下图中位于w’x+b=1或w’x+b=-1上的数据点，我们称之为<strong>支持向量</strong>（support vector），易知：对于所有的支持向量，它们恰好满足y<em>(w’x</em>+b)=1，而所有不是支持向量的点，有y<em>(w’x</em>+b)&gt;1。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6a838c4.png" alt="9.png"></p><h2 id="6-3-从原始优化问题到对偶问题"><a href="#6-3-从原始优化问题到对偶问题" class="headerlink" title="6.3 从原始优化问题到对偶问题"></a><strong>6.3 从原始优化问题到对偶问题</strong></h2><p>对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f6978cbb.png" alt="10.png"></p><p>即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的<strong>对偶问题</strong>，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：</p><pre><code>* 一是因为使用对偶问题更容易求解；* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。</code></pre><p>对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9332be7.png" alt="11.png"></p><p>上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f93321c5.png" alt="12.png"></p><p>由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9330967.png" alt="13.png"></p><p>这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。</p><p>（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9333e66.png" alt="14.png"></p><p>将上述结果代入L得到：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f935ae21.png" alt="15.png"></p><p>（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9338a9d.png" alt="16.png"></p><p>（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f93419ca.png" alt="17.png"></p><p>在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。</p><p><img src="https://i.loli.net/2018/10/17/5bc72f9353166.png" alt="18.png"></p><p>这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f933c947.png" alt="19.png">        </p><h2 id="6-4-核函数"><a href="#6-4-核函数" class="headerlink" title="6.4 核函数"></a><strong>6.4 核函数</strong></h2><p>由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用<strong>映射</strong>的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc72f934303e.png" alt="20.png"></p><p>按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：</p><p>（1）原对偶问题变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc68b3b.png" alt="21.png"></p><p>（2）原分类函数变为：<br>​    <img src="https://i.loli.net/2018/10/17/5bc730cc1b673.png" alt="22.png"></p><p>求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了<strong>核函数</strong>（Kernel）的概念。</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc49adc.png" alt="23.png"></p><p>因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效<strong>（低维计算，高维表现）</strong>，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：</p><p>（1）对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc173b2.png" alt="24.png"></p><p>（2）分类函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc05959.png" alt="25.png"></p><p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccc468c.png" alt="26.png"></p><p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccc541a.png" alt="27.png"></p><h2 id="6-5-软间隔支持向量机"><a href="#6-5-软间隔支持向量机" class="headerlink" title="6.5 软间隔支持向量机"></a><strong>6.5 软间隔支持向量机</strong></h2><p>前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有<strong>噪声</strong>的情形，噪声数据（<strong>outlier</strong>）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。</p><p><img src="https://i.loli.net/2018/10/17/5bc730ccce68e.png" alt="28.png"></p><p>为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了<strong>“软间隔”支持向量机</strong>的概念</p><pre><code>* 允许某些数据点不满足约束y(w&#39;x+b)≥1；* 同时又使得不满足约束的样本尽可能少。</code></pre><p>这样优化目标变为：</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc6c9fe.png" alt="29.png"></p><p>如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。</p><p><img src="https://i.loli.net/2018/10/17/5bc730cc5e5a9.png" alt="30.png"></p><p>支持向量机中的损失函数为<strong>hinge损失</strong>，引入<strong>“松弛变量”</strong>，目标函数与约束条件可以写为：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317aa3411.png" alt="31.png"></p><p>其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317a4c96e.png" alt="32.png"></p><p>按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317a6dff2.png" alt="33.png"></p><p>将w代入L化简，便得到其对偶问题：</p><p><img src="https://i.loli.net/2018/10/17/5bc7317ab6646.png" alt="34.png"></p><p>将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;6、支持向量机&quot;&gt;&lt;a href=&quot;#6、支持向量机&quot; class=&quot;headerlink&quot; title=&quot;6、支持向量机&quot;&gt;&lt;/a&gt;&lt;strong&gt;6、支持向量机&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔</summary>
      
    
    
    
    <category term="Machine learning" scheme="https://allen123321.github.io/categories/Machine-learning/"/>
    
    
    <category term="西瓜书" scheme="https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://allen123321.github.io/2021/07/29/hello-world/"/>
    <id>https://allen123321.github.io/2021/07/29/hello-world/</id>
    <published>2021-07-29T01:09:06.613Z</published>
    <updated>2021-08-12T03:20:48.845Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h3 id="发表文章"><a href="#发表文章" class="headerlink" title="发表文章"></a>发表文章</h3><p>新建文章</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo n <span class="string">&quot;我的博客&quot;</span> == hexo new <span class="string">&quot;我的博客&quot;</span></span><br></pre></td></tr></table></figure><p>发表草稿</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo p == hexo publish</span><br></pre></td></tr></table></figure><p>生成静态文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g == hexo generate</span><br></pre></td></tr></table></figure><p>启动服务预览<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s == hexo server</span><br></pre></td></tr></table></figure></p><p>部署到远程<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d == hexo deploy</span><br></pre></td></tr></table></figure></p><h3 id="清理缓存"><a href="#清理缓存" class="headerlink" title="清理缓存"></a>清理缓存</h3><p>清除缓存文件 (db.json) 和已生成的静态文件 (public)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure></p><h3 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h3><p>查看Hexo运行版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo version</span><br></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
