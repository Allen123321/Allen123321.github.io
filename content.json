{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"Repositories","date":"2021-07-30T05:02:20.384Z","updated":"2021-07-30T03:46:36.204Z","comments":false,"path":"repository/index.html","permalink":"http://example.com/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-07-30T05:04:02.134Z","updated":"2021-07-30T03:46:36.204Z","comments":true,"path":"links/index.html","permalink":"http://example.com/links/index.html","excerpt":"","text":""}],"posts":[{"title":"西瓜书第1章","slug":"西瓜书第1章","date":"2021-08-04T00:04:25.000Z","updated":"2021-08-04T02:07:52.521Z","comments":true,"path":"2021/08/04/西瓜书第1章/","link":"","permalink":"http://example.com/2021/08/04/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC1%E7%AB%A0/","excerpt":"","text":"第一章学习笔记1、引言傍晚小街路面上沁出微雨后的湿润，和熙的细风吹来，抬头看看天边的晚霞，嗯，明天又是一个好天气。走到水果摊旁，挑了个根蒂蜷缩、敲起来声音浊响的青绿西瓜，一边满心期待着皮薄肉厚瓢甜的爽落感，一边愉快地想着，这学期狠下了工夫，基础概念弄得清清楚楚，算法作业也是信手拈来，这门课成绩一定差不了！ 1.1、机器学习的定义正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。 另一本经典教材的作者Mitchell给出了一个形式化的定义，假设： P：计算机程序在某任务类T上的性能。 T：计算机程序希望实现的任务类。 E：表示经验，即历史的数据集。 若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。 1.2、机器学习的一些基本术语假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)，(色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义： 所有记录的集合为：数据集。 每一条记录为：一个实例（instance）或样本（sample）。 例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。 对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。 一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义： 所有训练样本的集合为：训练集（trainning set），[特殊]。 所有测试样本的集合为：测试集（test set），[一般]。 机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。 西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义： 预测值为离散值的问题为：分类（classification）。 预测值为连续值的问题为：回归（regression）。 我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义： 训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。 训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类（clustering）和关联规则。 训练training、验证validation/develop、测试testing 的作用 学得模型后，使用其进行预测的过程称为测试。 为什么要将数据集划分为三个部分？三个部分的作用？三个部分数据集的比例应如何设定？ 另外一种常见的数据集划分方法是将数据集划分为两个部分（训练集和测试集），这种划分方法存在的问题在于，模型利用训练集数据进行训练，测试集数据进行模型泛化性测试。但我们不能利用测试集测试的bad case或者根据测试集的测试精度调整模型的参数。这是因为对于模型来说，数据集应该是只有训练集可见的，其他数据均不可见，如果利用测试集的结果调整模型相对于模型也”看到了“测试集的数据。将数据集划分为是独立同分布的三个部分就可以解决这个问题，将训练集数据用于模型训练，验证集（开发集）数据用于模型调参，测试集数据用于验证模型泛化性。 1.3 假设空间 归纳induction和演绎deduction 归纳：从特殊到一般的“泛化”过程，从具体事实归结一般规律。 演绎：从一般到特殊的“特化”过程，从基础原理推演具体情况。 假设hypothesis space、版本空间hypothesis space 假设空间：监督学习（supervised learning）的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出 一个好的预测。模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间(hypothesis space)。我们也可以将学习过程看作一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集”匹配”的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确定，假设空间及其规模大小就确定了。 版本空间：与训练集一致的“假设集合”。 理解《机器学习》中的假设空间和版本空间 1.4、归纳偏好inductive bias 在学习过程中对某种类型假设的偏好。有效的机器学习算法必有其归纳偏好。基于某种领域的知识而产生的归纳偏好，与特征选择不同，特征选择是基于训练样本分析选择有效的特征或者更加重视某类特征。 归纳偏好与问题本身匹配，偏好什么能让模型更好。 “奥卡姆剃刀”(Occam’s razor)是一种常用的、自然科学研究中最基本的原则，用于确立”正确的”偏好——即”若有多个假设与观察一致，则选最简单的那个“。 没有免费午餐定理No Free Lunch Theorem -NFL 根据奥卡姆剃刀原则，下图(a)中A曲线的泛化能力要比曲线B强，但可能存在图(b)这样的情况，B曲线更好地满足测试集样本。也就是说，针对某种分布的样本空间（某种问题），算法A可能优于B，而针对另外一种分布的样本空间（另外一种问题），算法B可能优于A，这就引出了没有免费午餐定理。 没有免费午餐定理表明：若考虑所有潜在的问题，所有学习算法的误差期望相同，也就是所有算法都一样好。因此，脱离具体问题空谈什么学习算法更好没有意义，在某些问题上表现好的学习算法在另外一些问题上可能不尽人意。学习算法自身的归纳偏好与目标问题是否匹配往往起决定作用。","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"http://example.com/categories/Machine-learning/"}],"tags":[{"name":"西瓜书","slug":"西瓜书","permalink":"http://example.com/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]},{"title":"西瓜书第6章","slug":"ML-xgs-svm","date":"2021-07-29T02:06:02.000Z","updated":"2021-08-03T23:58:30.373Z","comments":true,"path":"2021/07/29/ML-xgs-svm/","link":"","permalink":"http://example.com/2021/07/29/ML-xgs-svm/","excerpt":"","text":"6、支持向量机支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。 6.1 函数间隔与几何间隔对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示： 对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。 6.1.1 函数间隔在超平面w’x+b=0确定的情况下，|w’x*+b|能够代表点x距离超平面的远近，易知：当w’x+b&gt;0时，表示x在超平面的一侧（正类，类标为1），而当w’x+b&lt;0时，则表示x在超平面的另外一侧（负类，类别为-1），因此（w’x+b）y* 的正负性恰能表示数据点x*是否被分类正确。于是便引出了函数间隔的定义（functional margin）: 而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔： 可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念–几何间隔（geometrical margin）。 6.1.2 几何间隔几何间隔代表的则是数据点到超平面的真实距离，对于超平面w’x+b=0，w代表的是该超平面的法向量，设x为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x=x-r(w/||w||)，又x在超平面上，即w’x+b=0，代入即可得： 为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义： 从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。 6.2 最大间隔与支持向量通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为： 一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为： 对于y(w’x+b)=1的数据点，即下图中位于w’x+b=1或w’x+b=-1上的数据点，我们称之为支持向量（support vector），易知：对于所有的支持向量，它们恰好满足y*(w’x*+b)=1，而所有不是支持向量的点，有y*(w’x*+b)&gt;1。 6.3 从原始优化问题到对偶问题对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为： 即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的对偶问题，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因： * 一是因为使用对偶问题更容易求解； * 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。 对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数： 上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于： 由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题： 这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。 （1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出： 将上述结果代入L得到： （2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。 （3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。 在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。 这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足： 6.4 核函数由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用映射的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为： 按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出： （1）原对偶问题变为： （2）原分类函数变为：​ 求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了核函数（Kernel）的概念。 因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效（低维计算，高维表现），从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为： （1）对偶问题： （2）分类函数： 因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件： 由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数： 6.5 软间隔支持向量机前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有噪声的情形，噪声数据（outlier）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。 为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了“软间隔”支持向量机的概念 * 允许某些数据点不满足约束y(w&#39;x+b)≥1； * 同时又使得不满足约束的样本尽可能少。 这样优化目标变为： 如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。 支持向量机中的损失函数为hinge损失，引入“松弛变量”，目标函数与约束条件可以写为： 其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到： 按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有： 将w代入L化简，便得到其对偶问题： 将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"http://example.com/categories/Machine-learning/"}],"tags":[{"name":"西瓜书","slug":"西瓜书","permalink":"http://example.com/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-07-29T01:09:06.613Z","updated":"2021-07-29T01:09:06.613Z","comments":true,"path":"2021/07/29/hello-world/","link":"","permalink":"http://example.com/2021/07/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"http://example.com/categories/Machine-learning/"}],"tags":[{"name":"西瓜书","slug":"西瓜书","permalink":"http://example.com/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]}