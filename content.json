{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"https://allen123321.github.io","root":"/"},"pages":[{"title":"友情链接","date":"2024-03-04T15:16:35.979Z","updated":"2021-07-30T03:46:36.204Z","comments":true,"path":"links/index.html","permalink":"https://allen123321.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2024-03-04T15:16:41.944Z","updated":"2021-07-30T03:46:36.204Z","comments":false,"path":"repository/index.html","permalink":"https://allen123321.github.io/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"计算机相关开源学习资源","slug":"计算机相关开源学习资源","date":"2024-03-05T14:18:56.000Z","updated":"2024-03-18T08:40:01.812Z","comments":true,"path":"2024/03/05/计算机相关开源学习资源/","link":"","permalink":"https://allen123321.github.io/2024/03/05/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3%E5%BC%80%E6%BA%90%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90/","excerpt":"","text":"开源学习资源计算机相关专业的开源学习资源(持续更新中…) 1. 计算机科学基础 计算机组成原理：解计算机的基本结构和工作原理 操作系统：学习计算机系统的管理和资源分配Linux 101 数据结构与算法：掌握各种数据结构的原理和算法设计技巧Hello 算法 计算机网络基础：了解网络的基本概念、协议和网络编程。中科大郑烇、杨坚 计算机网络（自顶向下方法 第7版) 2. 编程语言 C/C++：学习编程基础和面向对象的编程思想。B站视频:”黑马程序员匠心之作|C++教程从0到1入门编程” Java：掌握跨平台的应用开发。 Python：学习用于数据分析、机器学习等领域的高级编程语言。Python 100 days Web开发（HTML, CSS, JavaScript）：了解网页制作和前端开发技术。 3. 软件工程 软件工程基础：学习软件开发的生命周期、项目管理等。 设计模式：掌握常用的软件设计模式。 版本控制系统：了解如何使用Git等工具进行代码管理。 容器化技术：了解容器化的基本原理Docker — 从入门到实践 4. 数据库技术 数据库原理：学习数据库设计、SQL语言等。 数据库管理系统：掌握MySQL、Oracle、SQL Server等常用数据库的应用。 5. 人工智能与机器学习 数学基础线性代数MIT线性代数MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning, Spring 2018 人工智能基础：了解AI的基本概念和应用。 机器学习：学习算法模型和数据处理技术。西瓜书 深度学习：掌握深度神经网络等先进技术。李宏毅深度学习教程动手学深度学习Deep Learning with PyTorch 6. 安全与隐私 计算机安全：了解网络安全、系统安全的基本知识和防护措施。 加密技术：学习数据加密和安全传输的方法。","categories":[{"name":"OER","slug":"OER","permalink":"https://allen123321.github.io/categories/OER/"}],"tags":[{"name":"OpenEducationalResources","slug":"OpenEducationalResources","permalink":"https://allen123321.github.io/tags/OpenEducationalResources/"}]},{"title":"mysql mac M1 安装配置和简单命令","slug":"mysql","date":"2021-11-05T18:07:22.000Z","updated":"2021-11-05T18:57:08.298Z","comments":true,"path":"2021/11/05/mysql/","link":"","permalink":"https://allen123321.github.io/2021/11/05/mysql/","excerpt":"","text":"安装 1.去mysql官网上找到dmg版本下载 （下载地址），M1选择arm版本。傻瓜式安装完成, 注意安装到最后一步，设置密码; 2、点击苹果标志-&gt;系统偏好设置-&gt;找到mysql 图标-&gt;双击-&gt;点击”Start MySQL Server”-&gt;MySQL启动成功； 配置打开终端，输入mysql如果遇到zsh: command not found: mysql 在终端执行以下命令：12345678910111213141516171819#step 1 进入到用户目录cd ~#step 2 vim ~/.bashrc# 在该文件中添加：alias mysql=/usr/local/mysql/bin/ # 保存退出:wq# step 3 使配置文件生效：source ~/.bashrc 重启mysql 看是否生效，如果没有，按照下面步骤继续： 123456789101112131415# step 1 在~/.bash_profile文件后面加入指令vim ~/.bash_profile# 将以下指令加入文件source ~/.bashrc # 保存退出 :wq # step 2 使配置文件生效 source ~/.bash_profile 命令行操作数据库 1 进入数据库 12mysql -u root -p#此时输入安装时设置的密码 2 显示所有数据库列表 1mysql&gt; show databases; 3 创建数据库文件 1mysql&gt; create database data charset &#x27;utf8&#x27;; 4 打开数据库 12mysql&gt; use data; 5 显示数据库文件中所有的表 1mysql&gt; show tables; 6 创建表 1mysql&gt; create table student(id int,name varshar(20)); 7 显示表结构 1mysql&gt; desc student; 8 创建数据 1vmysql&gt; insert into student valuse(0,&quot;alex&quot;)； 9 查看表中数据 123vmysql&gt; select * from student；","categories":[{"name":"Tool","slug":"Tool","permalink":"https://allen123321.github.io/categories/Tool/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://allen123321.github.io/tags/mysql/"}]},{"title":"机器学习：拉格朗日对偶问题","slug":"机器学习：拉格朗日对偶问题","date":"2021-10-06T17:17:28.000Z","updated":"2021-10-11T09:05:18.000Z","comments":true,"path":"2021/10/06/机器学习：拉格朗日对偶问题/","link":"","permalink":"https://allen123321.github.io/2021/10/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/","excerpt":"","text":"1.什么是对偶问题？在线性规划的早期发展中最重要的发现就是对偶问题，即每一个线性规划问题（称为原始问题）都有一个与它对应的对偶线性规划问题（称为对偶问题）。 2.优化问题2.1 无约束的优化问题$\\mathop{min}f(x)$其中，$x=(x_{1},x_{2})$注意在图里画了等高线。此时$f(x)$ 在局部极小值点 $x^* =(x_{1}^*,x_{2}^*) $ 处的梯度必然为0，比较容易理解。这个梯度为零的条件是局部极小值点的必要条件。这样，优化问题的求解变成了对该必要条件解方程组。 2.2 带等式约束的优化问题$\\mathop{min}f(x)$s.t.$h(x)=0$ 与无约束的问题不同。我们所要求的极小值点被限制在曲线$h(x) = 0$上，我们将${x|h(x)=0}$称为可行域, 解只能在这个可行域里取。如下图所示，曲线$h(x) = 0$ (黑色实曲线）经过无约束极小值点（黑点）附近。那么满足约束的极小值点应该与黑点尽可能近。我们将$f(x)$ 的等高线不断放大，直到与曲线 $h(x) = 0$ 相切，切点即为所求。相切是关键，是极小值点的必要条件。 令其偏导为0，正好就得到拉格朗日条件。 如此，带等式约束的优化问题转化为了无约束的优化问题，只需要对拉格朗日条件解方程组即可。这里λ就是拉格朗日乘子，有多少个等式约束就有多少个拉格朗日乘子。 2.3 带不等式约束的优化问题$\\mathop{min} f(x)$$ s.t $$h(x) \\leq 0 $ 当只有一个不等式起作用时, 如我们把问题2里的等式约束$h(x) = 0$ 改为 $h(x) \\leq 0$, 如下图所示，可行域变成了阴影部分，最小值点还是切点，情况和问题2完全一样，只需要把不等号当做等号去求解即可。 参考链接知乎","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://allen123321.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"LaTex常用数学符号整理","slug":"LaTex常用数学符号整理","date":"2021-10-05T12:19:15.000Z","updated":"2021-10-05T16:56:25.000Z","comments":true,"path":"2021/10/05/LaTex常用数学符号整理/","link":"","permalink":"https://allen123321.github.io/2021/10/05/LaTex%E5%B8%B8%E7%94%A8%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7%E6%95%B4%E7%90%86/","excerpt":"","text":"在论文和博客的写作中，经常会用到Latex的语法来书写数学公式，一份详细的数学符号对照表必不可少。 1.常用公式 描述 写法 效果展示 上下标 a_0 = 3^2 $a_0 = 3^2$ 上下标（多字符） A_{ij} = 2^{i+j} $A_{ij} = 2^{i+j}$ 角度 A = 90^\\circ $A = 90^\\circ$ 上下方水平线 \\overline{m+n} / \\underline{m+n} $\\overline{m+n} / \\underline{m+n}$ 上下方大括号 \\overbrace{m+n} / \\underbrace{m+n} $\\overbrace{m+n} / \\underbrace{m+n}$ 向量 \\vec a $\\vec a$ 分数 \\frac{1}{2} $\\frac{1}{2}$ 求和 \\sum_{i=1}^n $\\sum_{i=1}^n$ 积分 \\int_0^{\\pi/2} $\\int_0^{\\pi/2}$ 累乘 \\prod_\\epsilon $\\prod_\\epsilon$ 优化 \\mathop{\\arg\\min}_{\\theta} $ \\mathop{\\arg\\min}_{\\theta}$ 上左箭头 \\overleftarrow{a+b} $\\overleftarrow{a+b}$ 上右箭头 \\overrightarrow{a+b} $\\overrightarrow{a+b}$ 下左箭头 \\underleftarrow{a+b} $\\underleftarrow{a+b}$ 下右箭头 \\underrightarrow{a+b} $\\underrightarrow{a+b}$ 2.矩阵12345\\begin&#123;pmatrix&#125; a&amp;b\\\\c&amp;d \\end&#123;pmatrix&#125; \\quad\\begin&#123;bmatrix&#125; a&amp;b\\\\c&amp;d \\end&#123;bmatrix&#125; \\quad\\begin&#123;Bmatrix&#125; a&amp;b\\\\c&amp;d \\end&#123;Bmatrix&#125; \\quad\\begin&#123;vmatrix&#125; a&amp;b\\\\c&amp;d \\end&#123;vmatrix&#125; \\quad\\begin&#123;Vmatrix&#125; a&amp;b\\\\c&amp;d \\end&#123;Vmatrix&#125; \\begin{pmatrix} a&b\\\\c&d \\end{pmatrix} \\quad \\begin{bmatrix} a&b\\\\c&d \\end{bmatrix} \\quad \\begin{Bmatrix} a&b\\\\c&d \\end{Bmatrix} \\quad \\begin{vmatrix} a&b\\\\c&d \\end{vmatrix} \\quad \\begin{Vmatrix} a&b\\\\c&d \\end{Vmatrix}2.1常规表示不同的列用符号 &amp; 分隔，行用 \\\\\\\\ 分隔。 123456A = \\begin&#123;pmatrix&#125; a_&#123;11&#125; &amp; a_&#123;12&#125; &amp; a_&#123;13&#125; \\\\ 0 &amp; a_&#123;22&#125; &amp; a_&#123;23&#125; \\\\ 0 &amp; 0 &amp; a_&#123;33&#125; \\end&#123;pmatrix&#125; A = \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ 0 & a_{22} & a_{23} \\\\ 0 & 0 & a_{33} \\end{pmatrix}矩阵中常常使用各种省略号，即 \\dots，\\vdots，\\ddots 等。 123456A =\\begin&#123;bmatrix&#125;a_&#123;11&#125; &amp; \\dots &amp; a_&#123;1n&#125; \\\\ &amp; \\ddots &amp; \\vdots \\\\0 &amp; &amp; a_&#123;nn&#125;\\end&#123;bmatrix&#125;_&#123;n \\times n&#125; A = \\begin{bmatrix} a_{11} & \\dots & a_{1n} \\\\ & \\ddots & \\vdots \\\\ 0 & & a_{nn} \\end{bmatrix}_{n \\times n}2.2分块矩阵分块矩阵可以理解为矩阵的嵌套。 123456789101112\\begin&#123;pmatrix&#125; \\begin&#123;matrix&#125; 1 &amp; 0\\\\ 0 &amp; 1 \\end&#123;matrix&#125; &amp; \\text&#123;\\Large 0&#125;\\\\ \\text&#123;\\Large 0&#125; &amp; \\begin&#123;matrix&#125; 1 &amp; 0\\\\ 0 &amp; -1 \\end&#123;matrix&#125;\\end&#123;pmatrix&#125; \\begin{pmatrix} \\begin{matrix} 1 & 0\\\\ 0 & 1 \\end{matrix} & \\text{0}\\\\ \\text{0} & \\begin{matrix} 1 & 0\\\\ 0 & -1 \\end{matrix} \\end{pmatrix}2.3矩阵的其他用法在行内公式中使用很小的矩阵，用的矩阵环境是 smallmatrix 环境。123456矩阵在行内表示 $ \\begin&#123;smallmatrix&#125; x &amp; -y \\\\ y &amp; x \\end&#123;smallmatrix&#125;$，但是没有括号。矩阵在行内表示 $ \\begin{smallmatrix} x &amp; -y \\\\ y &amp; x \\end{smallmatrix}$，但是没有括号。 用 \\substack 命令排版列矩阵，用以处理多行内容的插入。1\\sum_&#123;\\substack&#123;0&lt;i&lt;n \\\\0&lt;j&lt;i&#125;&#125; A_&#123;ij&#125; \\sum_{\\substack{0","categories":[{"name":"Tool","slug":"Tool","permalink":"https://allen123321.github.io/categories/Tool/"}],"tags":[{"name":"LaTex","slug":"LaTex","permalink":"https://allen123321.github.io/tags/LaTex/"}]},{"title":"Python基础 | Mac M1 安装配置PyQt5","slug":"Python基础-Mac-M1-安装配置PyQt5","date":"2021-10-04T16:57:48.000Z","updated":"2021-11-05T18:35:12.000Z","comments":true,"path":"2021/10/04/Python基础-Mac-M1-安装配置PyQt5/","link":"","permalink":"https://allen123321.github.io/2021/10/04/Python%E5%9F%BA%E7%A1%80-Mac-M1-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEPyQt5/","excerpt":"","text":"安装环境 Mac M1(macOS 11.5) PyCharm 2021.1.3 Python3 需要安装 qt sip PyQt5 开始安装网上有很多在pycharm中直接安装PyQt5的，但是并不适用于M1，在pycharm中找不到PyQt5的安装包。经过多次试错找到以下适合M1的安装方法。 1. 使用Homebrew安装 qt在终端中输入：1brew install qt在这里brew会重新安装一个python，我这里安装的是python3.9.7版本。 2. 使用Homebrew安装 sip这里不使用pip安装，是因为虽可成功安装sip，但是接下来用pip安装pyqt或者PyQt5会出错，未找到出错原因。我这里使用以下命令安装sip，终端中输入：1brew install sip 3.使用Homebrew安装Pyqt5终端中输入：1brew install pyqt5 检查安装情况终端中输入：1brew list可查看安装后的qt和sip，pyqt5，以及新的python。 配置pycharm1.切换到岗安装的python3.9环境中打开pycharm，重新配置Python Interpreter。Preferences—&gt; Python Interpreter 选择Homebrew路径下的python，添加。如下图所示 然后可以看到安装成功的pyqt5。 配置External Tools安装好开发工具后，我们可以在External Tools中配置些快捷工具方便我们的开发 首先配置QT Designer的快速启动工具：Preferences—&gt;Tools—&gt;External Tools中新增项,点击➕号添加。在终端中输入以下命令获取 Designer 地址：1brew list qt | grep Designer.app我的路径是/opt/homebrew/Cellar/qt/6.1.3/libexec/Designer.app, 如下图配置。配置参数：1234Name:随意配置Program：选择Designer可执行程序的路径Arguments：空Working directory：填当前项目工作目录$ProjiectFileDir$配置成功 如下所示： 配置PyUIC5现在我们去找到PyUIC5并配置它 1which pyuic5 12# 我的路径/opt/homebrew/bin/pyuic5 现在copy它，重复之前的步骤添加外部工具，我的参数配置： 1234Name:随意配置Program：/opt/homebrew/bin/pyuic5Arguments：-x $FileName$ -o $FileNameWithoutExtension$.pyWorking directory：$ProjiectFileDir$ 现在就大功告成了，使用方法见下图; 使用PyInstaller打包项目可以使用PyInstaller插件来进行打包（PyInstaller在不同系统下打包生成的都是当前系统的可执行文件，也就是说如果想要生成window可用的exe文件，可以到window环境下去打包） 1 安装PyInstaller在Pycharm中使用pip去安装PyInstaller 2 打包 在Pycharm的Terminal中执行：1pyinstaller -F 文件名.py","categories":[{"name":"Tool","slug":"Tool","permalink":"https://allen123321.github.io/categories/Tool/"}],"tags":[{"name":"PyQt5","slug":"PyQt5","permalink":"https://allen123321.github.io/tags/PyQt5/"}]},{"title":"李宏毅深度学习--深度学习和反向传播","slug":"li-deeplearning","date":"2021-08-22T16:15:15.000Z","updated":"2021-08-24T08:44:03.168Z","comments":true,"path":"2021/08/22/li-deeplearning/","link":"","permalink":"https://allen123321.github.io/2021/08/22/li-deeplearning/","excerpt":"","text":"深度学习的三个步骤我们都知道机器学习有三个step，对于deep learning其实也是3个步骤： Step1：神经网络（Neural network） Step2：模型评估（Goodness of function） Step3：选择最优函数（Pick best function） Step1：神经网络神经网络（Neural network）里面的节点，类似我们的神经元。 神经网络也可以有很多不同的连接方式，这样就会产生不同的结构（structure）在这个神经网络里面，我们有很多逻辑回归函数，其中每个逻辑回归都有自己的权重和自己的偏差，这些权重和偏差就是参数。 那这些神经元都是通过什么方式连接的呢？其实这些连接方式都是你手动去设计的。 完全连接前馈神经网络概念：前馈（feedforward）也可以称为前向，从信号流向来理解就是输入信号进入网络后，信号流动是单向的，即信号从前一层流向后一层，一直到输出层，其中任意两层之间的连接并没有反馈（feedback），亦即信号没有从后一层又返回到前一层。 当已知权重和偏差时输入$(1,-1)​$的结果 当已知权重和偏差时输入$(-1,0)$的结果 当输入0和0时，则得到0.51和0.85，所以一个神经网络如果权重和偏差都知道的话就可以看成一个函数，他的输入是一个向量，对应的输出也是一个向量。不论是做回归模型（linear model）还是逻辑回归（logistics regression）都是定义了一个函数集（function set）。我们可以给上面的结构的参数设置为不同的数，就是不同的函数（function）。这些可能的函数（function）结合起来就是一个函数集（function set）。这个时候你的函数集（function set）是比较大的，是以前的回归模型（linear model）等没有办法包含的函数（function），所以说深度学习（Deep Learning）能表达出以前所不能表达的情况。 全链接和前馈的理解 输入层（Input Layer）：1层 隐藏层（Hidden Layer）：N层 输出层（Output Layer）：1层 为什么叫全链接呢？ 因为layer1与layer2之间两两都有连接，所以叫做Fully Connect； 为什么叫前馈呢？ 因为现在传递的方向是由后往前传，所以叫做Feedforward。 深度的理解那什么叫做Deep呢？Deep = Many hidden layer。那到底可以有几层呢？这个就很难说了，以下是一些比较深的神经网络的例子 2012 AlexNet：8层 2014 VGG：19层 2014 GoogleNet：22层 2015 Residual Net：152层 101 Taipei：101层 随着层数变多，错误率降低，随之运算量增大，通常都是超过亿万级的计算。对于这样复杂的结构，我们一定不会一个一个的计算，对于亿万级的计算，使用loop循环效率很低。 这里我们就引入矩阵计算（Matrix Operation）能使得我们的运算的速度以及效率高很多。 隐藏层的理解通过隐藏层进行特征转换把隐藏层通过特征提取来替代原来的特征工程，这样在最后一个隐藏层输出的就是一组新的特征（相当于黑箱操作）而对于输出层，其实是把前面的隐藏层的输出当做输入（经过特征提取得到的一组最好的特征）然后通过一个多分类器（可以是softmax函数）得到最后的输出y。 神经网络的结构决定了函数集（function set），所以说网络结构（network structured）很关键。 有几个问题： 多少层？ 每层有多少神经元？这个问我们需要用尝试加上直觉的方法来进行调试。对于有些机器学习相关的问题，我们一般用特征工程来提取特征，但是对于深度学习，我们只需要设计神经网络模型来进行就可以了。对于语音识别和影像识别，深度学习是个好的方法，因为特征工程提取特征并不容易。 结构可以自动确定吗？有很多设计方法可以让机器自动找到神经网络的结构的，比如进化人工神经网络（Evolutionary Artificial Neural Networks）但是这些方法并不是很普及 。 我们可以设计网络结构吗？可以的，比如 CNN卷积神经网络（Convolutional Neural Network ） Step2: 模型评估对于模型的评估，我们一般采用损失函数来反应模型的好差，所以对于神经网络来说，我们采用交叉熵（cross entropy）函数来对$y$和$\\hat{y}​$的损失进行计算，调整参数，让交叉熵越小越好。 总体损失 对于损失，我们不单单要计算一笔数据的，而是要计算整体所有训练数据的损失，然后把所有的训练数据的损失都加起来，得到一个总体损失L。接下来就是在function set里面找到一组函数能最小化这个总体损失L，或者是找一组神经网络的参数$\\theta$，来最小化总体损失L Step3：选择最优函数梯度下降 具体流程： 给到 $\\theta$ (weight and bias) 先选择一个初始的 $\\theta^0$，计算 $\\theta^0$ 的损失函数（Loss Function）设一个参数的偏微分 计算完这个向量（vector）偏微分，然后就可以去更新的你 $\\theta$ 百万级别的参数（millions of parameters） 反向传播（Backpropagation）是一个比较有效率的算法，让你计算梯度（Gradient） 的向量（Vector）时，可以有效率的计算出来 BP主要用到了chain rule 反向传播 损失函数(Loss function)是定义在单个训练样本上的，也就是就算一个样本的误差，比如我们想要分类，就是预测的类别和实际类别的区别，是一个样本的，用L表示。 代价函数(Cost function)是定义在整个训练集上面的，也就是所有样本的误差的总和的平均，也就是损失函数的总和的平均，有没有这个平均其实不会影响最后的参数的求解结果。 总体损失函数(Total loss function)是定义在整个训练集上面的，也就是所有样本的误差的总和。也就是平时我们反向传播需要最小化的值。 对于$L(\\theta)$就是所有$l^n$的损失之和，所以如果要算每个$L(\\theta)$的偏微分，我们只要算每个$l^n$的偏微分，再把所有$l^n$偏微分的结果加起来就是$L(\\theta)$的偏微分，所以我们只计算每个$l^n​$的偏微分。 我们的目标是要求计算$\\frac{\\partial z}{\\partial w}$（Forward pass的部分）和计算$\\frac{\\partial l}{\\partial z}$ ( Backward pass的部分 )，然后把$\\frac{\\partial z}{\\partial w}$和$\\frac{\\partial l}{\\partial z}$相乘，我们就可以得到$\\frac{\\partial l}{\\partial w}$,所有我们就可以得到神经网络中所有的参数，然后用梯度下降就可以不断更新，得到损失最小的函数","categories":[{"name":"Deep learning","slug":"Deep-learning","permalink":"https://allen123321.github.io/categories/Deep-learning/"}],"tags":[{"name":"李宏毅","slug":"李宏毅","permalink":"https://allen123321.github.io/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"}]},{"title":"李宏毅机器学习--误差和梯度","slug":"li-error-and-gradient","date":"2021-08-20T07:54:16.000Z","updated":"2021-08-20T10:20:38.000Z","comments":true,"path":"2021/08/20/li-error-and-gradient/","link":"","permalink":"https://allen123321.github.io/2021/08/20/li-error-and-gradient/","excerpt":"","text":"1. Error的来源Error 的主要有两个来源，分别是 bias 和 variance。 1.1. bias 和 varianceError = Bias + VarianceError反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。举一个例子，一次打靶实验，目标是为了打到10环，但是实际上只打到了7环，那么这里面的Error就是3。具体分析打到7环的原因，可能有两方面：一是瞄准出了问题，比如实际上射击瞄准的是9环而不是10环；二是枪本身的稳定性有问题，虽然瞄准的是9环，但是只打到了7环。那么在上面一次射击实验中，Bias就是1,反应的是模型期望与真实目标的差距，而在这次试验中，由于Variance所带来的误差就是2，即虽然瞄准的是9环，但由于本身模型缺乏稳定性，造成了实际结果与模型期望之间的差距。知乎链接：机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？ 在一个实际系统中，Bias与Variance往往是不能兼得的。如果要降低模型的Bias，就一定程度上会提高模型的Variance，反之亦然。造成这种现象的根本原因是，我们总是希望试图用有限训练样本去估计无限的真实数据。当我们更加相信这些数据的真实性，而忽视对模型的先验知识，就会尽量保证模型在训练样本上的准确度，这样可以减少模型的Bias。但是，这样学习到的模型，很可能会失去一定的泛化能力，从而造成过拟合，降低模型在真实数据上的表现，增加模型的不确定性。相反，如果更加相信我们对于模型的先验知识，在学习模型的过程中对模型增加更多的限制，就可以降低模型的variance，提高模型的稳定性，但也会使模型的Bias增大。Bias与Variance两者之间的trade-off是机器学习的基本主题之一。 1.2. 考虑不同模型的方差用比较简单的模型，方差是比较小的（就像射击的时候每次的时候，每次射击的设置都集中在一个比较小的区域内）。如果用了复杂的模型，方差就很大，散布比较开。 这也是因为简单的模型受到不同训练集的影响是比较小的。 1.3. 考虑不同模型的偏差简单模型的偏差比较大，variance比较小。而复杂的模型，偏差比较小，variance比较大。 直观的解释：简单的模型函数集的space比较小，所以可能space里面就没有包含靶心，肯定射不中。而复杂的模型函数集的space比较大，可能就包含的靶心，只是没有办法找到确切的靶心在哪，但足够多的，就可能得到真正的 $\\bar{f}$。 1.3.1. 偏差v.s.方差 简单模型（左边）是偏差比较大造成的误差，这种情况叫做欠拟合，而复杂模型（右边）是方差过大造成的误差，这种情况叫做过拟合。 1.3.2. 过拟合，欠拟合如果模型没有很好的训练训练集，就是偏差过大，也就是欠拟合 如果模型很好的训练训练集，即再训练集上得到很小的错误，但在测试集上得到大的错误，这意味着模型可能是方差比较大，就是过拟合。 对于欠拟合和过拟合，是用不同的方式来处理的。 1.3.3. 偏差大-欠拟合此时应该重新设计模型。因为之前的函数集里面可能根本没有包含$f^*$。可以： 1.3.4. 方差大-过拟合简单粗暴的方法：更多的数据 但是很多时候不一定能做到收集更多的data。可以针对对问题的理解对数据集做调整。比如识别手写数字的时候，偏转角度的数据集不够，那就将正常的数据集左转15度，右转15度，类似这样的处理。 1.4. 交叉验证 图中public的测试集是已有的，private是没有的，不知道的。交叉验证 就是将训练集再分为两部分，一部分作为训练集，一部分作为验证集。用训练集训练模型，然后再验证集上比较，确实出最好的模型之后（比如模型3），再用全部的训练集训练模型3，然后再用public的测试集进行测试，此时一般得到的错误都是大一些的。 上述方法可能会担心将训练集拆分的时候分的效果比较差怎么办，可以用下面的方法。 1.5. N-折交叉验证将训练集分成N份，比如分成3份。 比如在三份中训练结果Average错误是模型1最好，再用全部训练集训练模型1。 2. 梯度下降（Gradient Descent）调整学习速率上图左边黑色为损失函数的曲线，假设从左边最高点开始，如果学习率调整的刚刚好，比如红色的线，就能顺利找到最低点。如果学习率调整的太小，比如蓝色的线，就会走的太慢，虽然这种情况给足够多的时间也可以找到最低点，实际情况可能会等不及出结果。如果 学习率调整的有点大，比如绿色的线，就会在上面震荡，走不下去，永远无法到达最低点。还有可能非常大，比如黄色的线，直接就飞出去了，更新参数的时候只会发现损失函数越更新越大。 上图右边，对学习率大小对损失函数的影响进行可视化。比如学习率太小（蓝色的线），损失函数下降的非常慢；学习率太大（绿色的线），损失函数下降很快，但马上就卡住不下降了；学习率特别大（黄色的线），损失函数就飞出去了；红色的就是差不多刚好，可以得到一个好的结果。 自适应学习率（Adaptive Learning Rate）举一个简单的思想：随着次数的增加，通过一些因子来减少学习率 通常刚开始，初始点会距离最低点比较远，所以使用大一点的学习率 update好几次参数之后呢，比较靠近最低点了，此时减少学习率 比如 $\\eta^t =\\frac{\\eta^t}{\\sqrt{t+1}}$，$t$ 是次数。随着次数的增加，$\\eta^t$ 减小 学习率不能是一个值通用所有特征，不同的参数需要不同的学习率 Adagrad 算法Adagrad 是什么？每个参数的学习率都把它除上之前微分的均方根。解释： 普通的梯度下降为： w^{t+1} \\leftarrow w^t -η^tg^t \\tag3\\eta^t =\\frac{\\eta^t}{\\sqrt{t+1}} \\tag4 $w$ 是一个参数 Adagrad 可以做的更好： w^{t+1} \\leftarrow w^t -\\frac{η^t}{\\sigma^t}g^t \\tag5g^t =\\frac{\\partial L(\\theta^t)}{\\partial w} \\tag6 $\\sigma^t$ :之前参数的所有微分的均方根，对于每个参数都是不一样的。 参数的更新过程： 随机梯度下降法之前的梯度下降： L=\\sum_n(\\hat y^n-(b+\\sum w_ix_i^n))^2 \\tag8\\theta^i =\\theta^{i-1}- \\eta\\triangledown L(\\theta^{i-1}) \\tag9而随机梯度下降法更快： 损失函数不需要处理训练集所有的数据，选取一个例子 $x^n$ L=(\\hat y^n-(b+\\sum w_ix_i^n))^2 \\tag{10}\\theta^i =\\theta^{i-1}- \\eta\\triangledown L^n(\\theta^{i-1}) \\tag{11}此时不需要像之前那样对所有的数据进行处理，只需要计算某一个例子的损失函数$L_n$，就可以快速update 梯度。 Feature Scaling 特征缩放比如有个函数： y=b+w_1x_1+w_2x_2 \\tag{12}两个输入的分布的范围很不一样，建议把他们的范围缩放，使得不同输入的范围是一样的。 为什么要这样做？上图左边是 $x_1$ 的scale比 $x_2$ 要小很多，所以当 $w_1$ 和 $w_2$ 做同样的变化时，$w_1$ 对 $y$ 的变化影响是比较小的，$x_2$ 对 $y$ 的变化影响是比较大的。 坐标系中是两个参数的error surface（现在考虑左边蓝色），因为 $w_1$ 对 $y$ 的变化影响比较小，所以 $w_1$ 对损失函数的影响比较小，$w_1$ 对损失函数有比较小的微分，所以 $w_1$ 方向上是比较平滑的。同理 $x_2$ 对 $y$ 的影响比较大，所以 $x_2$ 对损失函数的影响比较大，所以在 $x_2$ 方向有比较尖的峡谷。 上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。 对于左边的情况，上面讲过这种狭长的情形不过不用Adagrad的话是比较难处理的，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。 缩放的方法非常多，如： min-max标准化（min-max normalization） log函数转换 atan函数转换 z-score标准化—&gt;零均值规范化（zero-mena normalization，此方法比较常用） 模糊量化法。 参考 Codehttps://github.com/datawhalechina/leeml-notes/tree/master/docs/Homework/HW_1 参考资料 datawhale-李宏毅学习笔记 李宏毅机器学习视频链接","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"李宏毅","slug":"李宏毅","permalink":"https://allen123321.github.io/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"}]},{"title":"李宏毅机器学习--回归","slug":"li-Regression","date":"2021-08-18T07:23:26.000Z","updated":"2021-08-18T06:41:32.000Z","comments":true,"path":"2021/08/18/li-Regression/","link":"","permalink":"https://allen123321.github.io/2021/08/18/li-Regression/","excerpt":"","text":"1.回归的定义Regression 就是找到一个函数 function，通过输入特征 x，输出一个数值 Scalar。 2.应用举例股市预测（Stock market forecast） 输入：过去10年股票的变动、新闻咨询、公司并购咨询等 输出：预测股市明天的平均值 自动驾驶（Self-driving Car） 输入：无人车上的各个sensor的数据，例如路况、测出的车距等 输出：方向盘的角度 商品推荐（Recommendation） 输入：商品A的特性，商品B的特性 输出：购买商品B的可能性 Pokemon精灵攻击力预测（Combat Power of a pokemon）： 输入：进化前的CP值、物种（Bulbasaur）、血量（HP）、重量（Weight）、高度（Height） 输出：进化后的CP值 3.构建模型的步骤 Step 1: 模型假设 一元线性模型（单个特征） 例如：线性模型假设 $y = b + w·x$ 多元线性模型（多个特征） 例如：线性模型 Linear model ： Y = b + \\sum w_{i}x_{i} $x_{i}$ :就是各种特征(fetrures) $w_i$：各个特征的权重 $b$：偏移量 Step 2: 模型评估 Goodness of Function 训练数据：将10组原始数据在二维图中展示，图中的每一个点 $(x_{cp}^n,\\hat{y}^n)$ 对应着 进化前的CP值 和 进化后的CP值。 如何判断众多模型的好坏有了这些真实的数据，那我们怎么衡量模型的好坏呢？从数学的角度来讲，我们使用距离。求【进化后的CP值】与【模型预测的CP值】差，来判定模型的好坏。也就是使用损失函数（Loss function） 来衡量模型的好坏，统计10组原始数据 $\\left ( \\hat{y}^n - f(x_{cp}^n) \\right )^2$ 的和越小模型越好。 \\begin{aligned} L(f) & = \\sum_{n=1}^{10}\\left ( \\hat{y}^n - f(x_{cp}^n) \\right )^2，将【f(x) = y】, 【y= b + w·x_{cp}】代入 \\ & = \\sum_{n=1}^{10}\\left ( \\hat{y}^n - (b + w·x_{cp}) \\right )^2\\ \\end{aligned}最终定义 损失函数 Loss function：$L(w,b)= \\sum_{n=1}^{10}\\left ( \\hat{y}^n - (b + w·x_{cp}) \\right )^2$ 我们将 $w$, $b$ 在二维坐标图中展示，如图所示： 图中每一个点代表着一个模型对应的 $w$ 和 $b$ 颜色越深代表模型更优 Step 3: 最佳模型 Best Function 如何筛选最优的模型（参数w，b）已知损失函数是 $L(w,b)= \\sum_{n=1}^{10}\\left ( \\hat{y}^n - (b + w·x_{cp}) \\right )^2$ ，需要找到一个令结果最小的 $f^*$，在实际的场景中，我们遇到的参数肯定不止 $w$, $b$。 先从最简单的只有一个参数$w$入手，定义$w^* = arg \\underset{x}{\\operatorname{\\min}} L(w)$ 首先在这里引入一个概念 学习率 ：移动的步长，如下图中 $\\eta$ 步骤1：随机选取一个 $w^0$ 步骤2：计算微分，也就是当前的斜率，根据斜率来判定移动的方向 大于0向右移动（增加$w$） 小于0向左移动（减少$w$） 步骤3：根据学习率移动 重复步骤2和步骤3，直到找到最低点 步骤1中，我们随机选取一个 $w^0$，如图8所示，我们有可能会找到当前的最小值，并不是全局的最小值，这里我们保留这个疑问，后面解决。 解释完单个模型参数$w$，引入2个模型参数 $w$ 和 $b$ ， 其实过程是类似的，需要做的是偏微分。整理成一个更简洁的公式： 梯度下降推演最优模型的过程如果把 w 和 b在图形中展示： 每一条线围成的圈就是等高线，代表损失函数的值，颜色约深的区域代表的损失函数越小 红色的箭头代表等高线的法线方向 如何验证训练好的模型的好坏?使用训练集和测试集的平均误差等来验证模型的好坏. 4. 过拟合问题在模型上，我们还可以进一部优化，选择更复杂的模型，但是复杂的模型容易引起过拟合问题。在训练集上面表现更为优秀的模型，在测试集上效果反而变差了。（详细参考西瓜书第二章） 借用知乎上某用户对过拟合的一段解释：1234567891011121314过拟合发生的本质原因，是由于监督学习问题的不适应：在高中数学中我们知道，从n个（线性无关）方程可以解n个变量，解n+1个变量就会解不出。在监督学习中，往往数据（对应了方程）远远少于模型空间（对应了变量）。因此过拟合现象的发生，可以分解成以下三点：1. 有限的训练数据不能完全反映出一个模型的好坏，然而我们却不得不在这有限的数据上挑选模型，因此我们完全有可能挑选到在训练数据上表现很好而在测试数据上表现很差的模型，因为我们完全无法知道模型在测试数据上的表现。2. 如果模型空间很大，也就是有很多很多模型可以给我们挑选，那么挑到对的模型的机会就会很小。3. 与此同时，如果我们要在训练数据上表现良好，最为直接的方法就是要在足够大的模型空间中挑选模型，否则如果模型空间很小，就不存在能够拟以合数据很好的模型。由以上3点可见，要拟合训练数据，就要足够大的模型空间;用了足够大的模型空间，挑选到测试性能好的模型的概率就会下降。因此，就会出现训练数据拟合越好，测试性能越差的过拟合现象。过拟合现象有多种解释，+ 经典的是bias-variance decomposition，+ PAC-learning泛化界解释，这种解释是最透彻，最fundamental的;+ Bayes先验解释，这种解释把正则变成先验。另外值得一提的是，不少人会用”模型复杂度“替代上面我讲的“模型空间”。这其实是一回事，但“模型复杂度”往往容易给人一个误解，认为是一个模型本身长得复杂。例如5次多项式就要比2次多项式复杂，这是错的。因此我更愿意用“模型空间”，强调“复杂度”是候选模型的“数量”，而不是本事的“长相”。为什么正则化能够避免过拟合？：因为正则化就是控制模型空间的一种办法。参考西瓜书的：“正则化削减了（容易过拟合的那部分）假设空间，从而降低过拟合风险” 5. Regularization在损失函数中加入正则项，称之为正则化（Regularize）。 目的: 防止模型过拟合 原理: 在损失函数上加上某些规则（限制），缩小解空间，从而减少求出过拟合解的可能性 w 越小，表示 function较平滑的， function输出值与输入值相差不大 在很多应用场景中，并不是 w 越小模型越平滑越好，但是经验值告诉我们 w 越小大部分情况下都是好的。 b 的值接近于0 ，对曲线平滑是没有影响 正则化有多种方式，包括L0（向量中非零元素个数），L1（向量中元素绝对值之和），L2（向量的模）。但是L0范数的求解是个NP完全问题。而L1也能实现稀疏并且比L0有更好的优化求解特性而被广泛应用。L2范数指各元素平方和后开根的值，可令 $w$ 每个元素接近于0，虽然不如L1更彻底地降低模型复杂度，但是由于处处可微降低了计算难度 6. Code 莺尾花分类 李宏毅课程回归例子 Github相关代码，线性回归，分类，波士顿房价预测 参考资料 知乎正则化 datawhale-李宏毅学习笔记 李宏毅机器学习视频链接","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"李宏毅","slug":"李宏毅","permalink":"https://allen123321.github.io/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"}]},{"title":"李宏毅机器学习--机器学习介绍","slug":"李宏毅机器学习（机器学习介绍）","date":"2021-08-16T14:08:20.000Z","updated":"2021-08-18T06:40:55.752Z","comments":true,"path":"2021/08/16/李宏毅机器学习（机器学习介绍）/","link":"","permalink":"https://allen123321.github.io/2021/08/16/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%EF%BC%89/","excerpt":"","text":"机器学习介绍1. 人工智能、机器学习、深度学习之间的关系 人工智能是我们最终要达到的目的，为机器赋予人的智能 机器学习是我们达成目的的一种手段 深度学习是机器学习中的一种方法，它指的是涉及到深度神经网络的机器学习算法 三者的关系：如下图 2.机器学习的定义机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。本质上就是通过程序让机器具备学习的能力，大量数据来训练机器，最终得到一个学习后的模型。 在整个machine learning framework整个过程分成了三个步骤。第一个步骤就是找一个function，第二个步骤让machine可以衡量一个function是好还是不好，第三个步骤是让machine有一个自动的方法，有一个好演算法可以挑出最好的function。 机器学习其实只有三个步骤，这三个步骤简化了整个process。可以类比为：把大象放进冰箱。我们把大象塞进冰箱，其实也是三个步骤：把门打开；象塞进去；后把门关起来，然后就结束了。所以说，机器学习三个步骤，就好像是说把大象放进冰箱，也只需要三个步骤。 机器学习相关的技术 3.监督学习（supervised learning）定义：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程 特征：所用训练数据都是被标记过的 4.半监督学习（SSL）为了减少监督学习中给训练数据做标记的工作量，是监督学习与无监督学习相结合的一种学习方法 定义：半监督学习使用大量的未标记数据，以及同时使用标记数据，来进行模式识别工作。 特征：训练数据中有部分数据是有标记的，有部分数据没有标记 5. 迁移学习目的：将某个领域或任务上学习到的知识应用到不同的但相关的领域或问题中 主要思想：利用相关领域的知识完成目标领域的任务 意义： 数据的标签很难获取 从头建立模型是复杂和耗时的 6. 无监督学习（Unsupervised learning ，UL）定义：根据类别未知 (没有被标记)的训练样本解决模式识别中的各种问题 特征：训练数据都是无标记的 特点： 无监督学习没有明确的目的 无监督学习不需要给数据打标签 无监督学习无法量化效果 7. 结构化学习 Structured LearningStructured Learning不是一种具体的技术，只是一种描述问题，分析问题的方法。 具体地说，Structured Learning 中让机器输出的是要有结构性的。举例来说：在语音辨识里面，机器输入是声音讯号，输出是一个句子。句子是要很多词汇拼凑完成。它是一个有结构性的object。或者是说在机器翻译里面你说一句话，你输入中文希望机器翻成英文，它的输出也是有结构性的。或者你今天要做的是人脸辨识，来给机器看张图片，它会知道说最左边是长门，中间是凉宫春日，右边是宝玖瑠。然后机器要把这些东西标出来，这也是一个structure learning问题。 8. 强化学习（Reinforcement Learning, RL）定义：强化学习是智能体（Agent）以“试错”的方式进行学习，通过与环境进行交互获得的奖赏指导行为，目标是使智能体获得最大的奖赏 基本模型：设置一个类似游戏的情景，算法会通过反复试验的方法来执行动作，并解决问题。对每一次试验与执行的动作，会设置对应的奖励，有时候也会有惩罚，算法最终的目标就是找到能够使奖励最大化的行动方案。 若我们用Alpha Go当做例子时，supervised learning就是告诉机器：看到这个盘式你就下“5-5”，看到这个盘式你就下“3-3” reinforcement learning的意思是：机器跟对手互下，机器会不断的下棋，最后赢了，机器就会知道下的不错，但是究竟是哪里可以使它赢，它其实是不知道的。我们知道Alpha Go其实是用监督学习加上reinforcement learning去学习的。先用棋谱做监督学习，然后在做reinforcement learning，但是reinforcement learning需要一个对手，如果使用人当对手就会很让费时间，所以机器的对手是另外一个机器。 为什么我们需要学习机器学习?机器学习为我们今天使用的许多服务提供了驱动力，如优酷，淘宝，今日头条的推荐系统; 百度和必应等搜索引擎;微博和微信这样的社交媒体; Siri和天猫精灵这样的语音助理，这样的名单还很长。 所有这些例子都表明机器学习在当今数据丰富的世界中已经开始发挥关键的作用。机器可以帮助我们筛选出有助于获得重大突破的有用信息，我们已经看到这种技术在各行各业中的广泛应用，如金融，医疗，保险，制造，转型变革等。 有了自动化机器学习，数据科学家可以通过自动执行重复性任务来提高工作效率。这使他们能够更多地关注解决问题本身而不是建模过程，并加快整个机器学习过程。传统机器学习整个过程中都需要人工干预，自动化机器学习流程还有助于避免人为错误。最终，自动化机器学习通过让每个人都可以使用机器学习，甚至是那些在这个领域没有专业知识的人，从而使AI变得更加普及。 参考资料 视频链接：https://www.bilibili.com/video/BV1Ht411g7Ef 开源笔记datawhale ：https://github.com/datawhalechina/leeml-notes","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"李宏毅","slug":"李宏毅","permalink":"https://allen123321.github.io/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"}]},{"title":"西瓜书第5章","slug":"西瓜书第5章","date":"2021-08-14T09:33:00.000Z","updated":"2021-09-27T15:30:19.700Z","comments":true,"path":"2021/08/14/西瓜书第5章/","link":"","permalink":"https://allen123321.github.io/2021/08/14/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC5%E7%AB%A0/","excerpt":"","text":"神经网络在机器学习中，神经网络一般指的是“神经网络学习”，是机器学习与神经网络两个学科的交叉部分。所谓神经网络，目前用得最广泛的一个定义是“神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应”。 1.神经元模型神经网络中最基本的单元是神经元模型（neuron）。在生物神经网络的原始机制中，每个神经元通常都有多个树突（dendrite），一个轴突（axon）和一个细胞体（cell body），树突短而多分支，轴突长而只有一个；在功能上，树突用于传入其它神经元传递的神经冲动，而轴突用于将神经冲动传出到其它神经元，当树突或细胞体传入的神经冲动使得神经元兴奋时，该神经元就会通过轴突向其它神经元传递兴奋。神经元的生物学结构如下图所示： 一直沿用至今的“M-P神经元模型”正是对这一结构进行了抽象，也称“阈值逻辑单元“，其中树突对应于输入部分，每个神经元收到n个其他神经元传递过来的输入信号，这些信号通过带权重的连接传递给细胞体，这些权重又称为连接权（connection weight）。细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和，或者说累积电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数（activation function）的处理，产生输出从轴突传送给其它神经元。M-P神经元模型如下图所示： 与线性分类十分相似，神经元模型最理想的激活函数也是阶跃函数，即将神经元输入值与阈值的差值映射为输出值1或0，若差值大于零输出1，对应兴奋；若差值小于零则输出0，对应抑制。但阶跃函数不连续，不光滑，故在M-P神经元模型中，也采用Sigmoid函数来近似， Sigmoid函数将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为挤压函数（squashing function）。 将多个神经元按一定的层次结构连接起来，就得到了神经网络。它是一种包含多个参数的模型，比方说10个神经元两两连接，则有100个参数需要学习（每个神经元有9个连接权以及1个阈值），若将每个神经元都看作一个函数，则整个神经网络就是由这些函数相互嵌套而成。 2 感知机与多层网络感知机（Perceptron）是由两层神经元组成的一个简单模型，但只有输出层是M-P神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。这样一来，感知机与之前线性模型中的对数几率回归的思想基本是一样的，都是通过对属性加权与另一个常数求和，再使用sigmoid函数将这个输出值压缩到0-1之间，从而解决分类问题。不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。 给定训练集，则感知机的n+1个参数（n个权重+1个阈值）都可以通过学习得到。阈值Θ可以看作一个输入值固定为-1的哑结点的权重ωn+1，即假设有一个固定输入xn+1=-1的输入层神经元，其对应的权重为ωn+1，这样就把权重和阈值统一为权重的学习了。简单感知机的结构如下图所示： 感知机只拥有一层功能激活神经元，学习能力有限，如果问题是线性可分的，则可以通过感知机的学习获得收敛（converge）,从而求出适当的权向量。否则感知机学习过程将会发生振荡。 3 误差逆传播算法（error BackPropagation, BP） 给定训练集$ D = {(x_{1},y_{1}),(x_{2},y_{2}),(x_{m},y_{m})},x_{i} \\in \\mathbb{R}^{d},y_{i} \\in \\mathbb{R}^{l}$ , 即输入示例有d个属性描述，输出l维实值向量。上图为一个单隐层前馈神经网络的拓扑结构，BP神经网络算法也使用梯度下降法（gradient descent），以单个样本的均方误差的负梯度方向对权重进行调节。 可以看出：BP算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的均方误差，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。BP算法基本的推导过程与感知机的推导过程原理是相同的。 BP算法工作流程对每个训练样例，BP算法执行以下操作：先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；然后计算输出层的误差，再将误差逆向传播至隐层神经网元，最后根据隐层神经元的误差来对连接权和阈值进行调整。该迭代过程循环进行，直到到达某些停止条件为止。 4 全局最小与局部极小模型学习的过程实质上就是一个寻找最优参数的过程，例如BP算法试图通过最速下降来寻找使得累积经验误差最小的权值与阈值，在谈到最优时，一般会提到局部极小（local minimum）和全局最小（global minimum）。 局部极小可以有多个，而全局最小只有一个。全局最小一定是局部极小，但局部最小却不一定是全局最小。显然在很多机器学习算法中，都试图找到目标函数的全局最小。梯度下降法的主要思想就是沿着负梯度方向去搜索最优解，负梯度方向是函数值下降最快的方向，若迭代到某处的梯度为0，则表示达到一个局部最小，参数更新停止。因此在现实任务中，通常使用以下策略尽可能地去接近全局最小。 以多组不同参数值初始化多个神经网络，按标准方法训练，迭代停止后，取其中误差最小的解作为最终参数。 使用“模拟退火”技术。 使用随机梯度下降，即在计算梯度时加入了随机因素，使得在局部最小时，计算的梯度仍可能不为0，从而迭代可以继续进行。 5 常见的神经网络 RBF网络（Radial Basis Function） ART网络（Adaptive Resonance Theory） SOM网络（self-organizing map） 级联相关网络 Elman网络 Boltzmann网络 6 深度学习理论上，参数越多，模型复杂度就越高，容量（capability）就越大，从而能完成更复杂的学习任务。深度学习（deep learning）正是一种极其复杂而强大的模型。 怎么增大模型复杂度呢？两个办法，一是增加隐层的数目，二是增加隐层神经元的数目。前者更有效一些，因为它不仅增加了功能神经元的数量，还增加了激活函数嵌套的层数。但是对于多隐层神经网络，经典算法如标准BP算法往往会在误差逆传播时发散（diverge），无法收敛达到稳定状态。 那要怎么有效地训练多隐层神经网络呢？一般来说有以下两种方法： 无监督逐层训练（unsupervised layer-wise training）：每次训练一层隐节点，把上一层隐节点的输出当作输入来训练，本层隐结点训练好后，输出再作为下一层的输入来训练，这称为预训练（pre-training）。全部预训练完成后，再对整个网络进行微调（fine-tuning）训练。一个典型例子就是深度信念网络（deep belief network，简称DBN）。这种做法其实可以视为把大量的参数进行分组，先找出每组较好的设置，再基于这些局部最优的结果来训练全局最优。 权共享（weight sharing）：令同一层神经元使用完全相同的连接权，典型的例子是卷积神经网络（Convolutional Neural Network，简称CNN）。这样做可以大大减少需要训练的参数数目。 深度学习可以理解为一种特征学习（feature learning）或者表示学习（representation learning），无论是DBN还是CNN，都是通过多个隐层来把与输出目标联系不大的初始输入转化为与输出目标更加密切的表示，使原来只通过单层映射难以完成的任务变为可能。即通过多层处理，逐渐将初始的“低层”特征表示转化为“高层”特征表示，从而使得最后可以用简单的模型来完成复杂的学习任务。","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"西瓜书","slug":"西瓜书","permalink":"https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]},{"title":"西瓜书第4章","slug":"西瓜书第4章","date":"2021-08-12T08:05:28.000Z","updated":"2021-08-17T08:00:33.608Z","comments":true,"path":"2021/08/12/西瓜书第4章/","link":"","permalink":"https://allen123321.github.io/2021/08/12/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC4%E7%AB%A0/","excerpt":"","text":"西瓜书第4章 决策树1. 算法原理决策树是基于树结构对问题进行决策或判定的过程。 决策过程中提出的判定问题（内部节点）是对某个属性的“测试”，每个测试的结果可以导出最终结论（叶节点）或导出进一步判定问题（下一层内部节点，其考虑范围是在上次决策结果的限定范围之内）。 算法核心：选取划分条件（划分属性）。 最终目的：样本划分越“纯”越好。 在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话： 123456789女儿：多大年纪了？母亲：26。女儿：长的帅不帅？母亲：挺帅的。女儿：收入高不？母亲：不算很高，中等情况。女儿：是公务员不？母亲：是，在税务局上班呢。女儿：那好，我去见见。 这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。 在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知： * 每个非叶节点表示一个特征属性测试。 * 每个分支代表这个特征属性在某个值域上的输出。 * 每个叶子节点存放一个类别。 * 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。 2. 决策树的构造决策树的构造是一个递归的过程，有三种情形会导致递归返回： 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别； 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别； 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。 算法基本流程伪码 可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。 3. ID3算法ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为： Ent(D) = - \\sum_{k=1}^{|y|} P_{k} log_{2} P_{k}信息熵值越大越混乱。 假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。 Gain(D,a) = Ent(D) - \\sum_{v=1}^V \\frac{|D^v|}{|D|} Ent(D^v)信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。 4. C4.5算法ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为： Gain\\_ratio(D,a) = \\frac{Gain(D,a)}{IV(a)}其中： IV(a) = - \\sum_{v = 1}^{V} \\frac{|D^v|}{|D|} log_{2}\\frac{|D^v|}{|D|}IV(a) 称为属性a的”固有值”。属性a的可能取值数目越多(即越大)，则IV(a)的值通常会越大。 C4.5 算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。 5. CART算法CART是Classification and Regression Tree的简称，这是一种著名的决策树学习算法，分类和回归任务都可用。其生成的决策树为二叉树。 CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，基尼指数定义如下： Gini(D) = \\sum_{k=1}^{|y|} \\sum_{k^{'} \\neq k} P_{k}P{k^{'}} \\\\ = 1- \\sum_{k=1}^{|y|} P_{k}^2任取两个样本不一致的概率，越小表示集合越纯 进而，使用属性α划分后的基尼指数为： Gini\\_index(D,a) = \\sum_{v=1}^{V} \\frac{|D^v|}{|D|} Gini(D^v)选择基尼指数最小的属性作为最优划分属性： a_{*} = \\mathop{\\arg\\min}_{a \\in A} Gini\\_index(D,a)算法流程 对每个属性a的每个可能取值v，将数据集D分为$a=v$和​​$a \\neq v $两部分来计算基尼指数，即： Gini\\_index(D,a) = \\frac{|D^{a=v}|}{|D|} Gini(D^{a = v}) + \\frac{|D^{a \\neq v}|}{|D|} Gini(D^{a \\neq v}) 选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点； 重复以上两步，直至满足停止条件。 6. 剪枝处理剪枝(pruning)是决策树学习算法对付”过拟合”的主要手段。，为了尽可能正确分类训练样本，有时会造成决策树分支过多，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。可通过主动去掉一些分支来降低过拟合的风险。 剪枝基本策略包括”预剪枝“(prepruning)和”后剪枝“(post”pruning)。 可以用交叉验证的方法，即预留一部分数据用作”验证集”以进行性 能评估，以判断剪枝是否能带来算法泛化性能的提升。 预剪枝 预剪枝指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。 优点预剪枝使得决策树的很多分支都没有”展开“，可以降低过拟合的风险；显著减少决策树的训练时间开销和测试时间开销。 缺点有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显 著提高； 预剪枝基于”贪心”本质禁止这些分支展开，可能导致欠拟合。 后剪枝 后剪枝先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。 根据奥卡姆剃刀准则，剪枝后的模型更好。因此，后剪枝下，决策树算法在验证集精度虽无提高的情况中会进行剪枝。 优点一般情形下，后剪枝决策树的欠拟合风险很小，泛化能往往优于预剪枝决策树； 缺点后剪枝过程是在生成完全决策树之后进行的，并且要白底向上对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。 7. 连续值与缺失值处理对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。 选择最大信息增益的划分点作为最优划分点。Gain(D,a) = \\mathop{\\max}_{t \\in T_{a}} Gain(D,a,t) \\\\ = \\mathop{\\max}_{t \\in T_{a}} Ent(D) - \\sum_{\\lambda \\in {-,+}} \\frac{|D_{t}^{\\lambda}|}{|D|} Ent(D_{t}^{\\lambda}) 现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义： \\rho = \\frac{\\sum_{x \\in \\widetilde{D}} W_{x}}{\\sum_{x \\in D} W_{x}}\\widetilde{\\rho}_{k} = \\frac{\\sum_{x \\in \\widetilde{D}_{k}} W_{x}}{\\sum_{x \\in \\widetilde{D}} W_{x}} , (1 \\leq k \\leq |Y|)\\widetilde{\\gamma}_{v} = \\frac{\\sum_{x \\in \\widetilde{D^{v}}} W_{x}}{\\sum_{x \\in \\widetilde{D}} W_{x}} , (1 \\leq \\nu \\leq V)对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即： 对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为： 参考代码Decision Tree（纯Python实现）决策树iris_data决策树实现mnist数据集 参考文献《机器学习》——周志华 【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集 周志华《机器学习》的学习笔记","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"西瓜书","slug":"西瓜书","permalink":"https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]},{"title":"西瓜书第3章","slug":"西瓜书第3章","date":"2021-08-06T12:51:47.000Z","updated":"2021-08-07T03:31:17.406Z","comments":true,"path":"2021/08/06/西瓜书第3章/","link":"","permalink":"https://allen123321.github.io/2021/08/06/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC3%E7%AB%A0/","excerpt":"","text":"第三章 线性模型3、线性模型谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。 3.1 线性回归线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000—&gt;13亿…2016—&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）—&gt;15k。 有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理： 若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。 若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。 （1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance 欧氏距离），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示： （2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成： 通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式： 同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。 另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示： 更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。 3.2 线性几率回归回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。 若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面列出求解的思路。 3.3 线性判别分析线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示： 想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。 类内散度矩阵（within-class scatter matrix） 类间散度矩阵(between-class scaltter matrix) 因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。 从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。 若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。​ 3.4 多分类学习现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。 OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。 OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。 MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。 3.5 类别不平衡问题类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种： 在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。 在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。 直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。 Code 线性回归实例python代码实现 Iris数据集线性分类sklearn boston housing 数据集线性回归实例 线性回归实例 参考文献《机器学习》——周志华 【吃瓜教程】《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集 Datawhale 周志华《机器学习》的学习笔记","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"西瓜书","slug":"西瓜书","permalink":"https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]},{"title":"西瓜书第2章","slug":"西瓜书第2章","date":"2021-08-05T07:04:47.000Z","updated":"2021-08-05T06:32:06.969Z","comments":true,"path":"2021/08/05/西瓜书第2章/","link":"","permalink":"https://allen123321.github.io/2021/08/05/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC2%E7%AB%A0/","excerpt":"","text":"第二章学习笔记模型的评估与选择2.1 误差与过拟合我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义： 在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。 在测试集上的误差称为测试误差（test error）。 学习器在所有新样本上的误差称为泛化误差（generalization error）。 我们实际希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义： 学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。 学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。 在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。 2.2 评估方法在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。 因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why： 假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。 2.2.1 训练集与测试集的划分方法如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法： 留出法 （Hold-out） 将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样（stratified sampling）保持S与T内的样本各个类别的比例和原分布相似。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。 案例代码: 利用sklearn的train_test_split函数进行数据集划分(基于留出法的原理)123456789101112131415161718from sklearn.model_selection import train_test_splitfrom sklearn import datasetsimport pandas as pd&#x27;&#x27;&#x27;载入数据&#x27;&#x27;&#x27;X,y = datasets.load_iris(return_X_y=True)&#x27;&#x27;&#x27;采取分层抽样时的数据集分割&#x27;&#x27;&#x27;X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y)&#x27;&#x27;&#x27;打印各个数据集的形状&#x27;&#x27;&#x27;print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)&#x27;&#x27;&#x27;打印训练集中因变量的各类别数目情况&#x27;&#x27;&#x27;print(pd.value_counts(y_train))&#x27;&#x27;&#x27;打印验证集集中因变量的各类别数目情况&#x27;&#x27;&#x27;print(pd.value_counts(y_test))交叉验证法将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪…∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。 利用sklearn的KFold函数划分数据集：12345678910from sklearn.model_selection import KFoldimport numpy as npX = np.random.randint(1,10,20)kf = KFold(n_splits=5)for train,test in kf.split(X): print(train,&#x27;\\n&#x27;,test) 与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。 利用sklearn的LeaveOneOut函数划分数据集：123456789from sklearn.model_selection import LeaveOneOutimport numpy as npX = np.random.randint(1,10,5)kf = LeaveOneOut()for train,test in kf.split(X): print(train,&#x27;\\n&#x27;,test) 自助法 我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。 自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为： 这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。 自助法-数据集划分1234567891011121314151617181920212223242526import numpy as np# 随机产生我们的数据集x = np.random.randint(-10, 10, 10) # 前两个参数表示范围，第三个参数表示个数index = [i for i in range(len(x))] # 数据集的下标train_set = [] # 训练集train_index = [] # 用于记录训练集各个元素的下标test_set = [] # 测试集test_index = [] # 用于记录测试集各个元素的下标# 进行m次放回抽样for i in range(len(x)): train_index.append(np.floor(np.random.random()*len(x)))# 计算D\\D&#x27;test_index = list(set(index).difference(set(train_index)))# 取数，产生训练/测试集for i in range(len(train_index)): train_set.append(x[int(train_index[i])]) # 这里记得强制转换为int型，否则会报错for i in range(len(test_index)): test_set.append(x[int(test_index[i])])# 打印结果进行验证print(&quot;data set: &quot;, x)print(&quot;train set: &quot;, train_set)print(&quot;test set: &quot;, test_set) 顺序约束采样 对于具有强顺序约束的序列（比如时间序列）的数据类型，前后相邻的数据关联程度很高，在数据分割时不能打乱顺序随机采样，即不能破坏序列的连续性。 sklearn中提供了TimeSeriesSplit函数用于分割这种类型的数据123456789from sklearn.model_selection import TimeSeriesSplitimport numpy as npX = np.random.randint(1,10,20)kf = TimeSeriesSplit(n_splits=4)for train,test in kf.split(X): print(train,&#x27;\\n&#x27;,test) 2.2.2 调参常用的方法包括网格法，利用一定的步长在参数的值域范围内探索参数最优解，在开始的时候可以选择较大的步长，然后逐步缩小步长逼近最优解。 将数据集划分为三个部分：训练集、验证集、测试集。将训练集数据用于模型训练，验证集（开发集）数据用于模型调参，测试集数据用于验证模型泛化性。 给定包含个样本的数据集，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练、模型。因此，在模型选择完成后，学习算法和参数配置己选定，此时应该用数据集重新训练模型。这个模型在训练过程中使用了所有个样本，这才是我们最终提交给用户的模型。 2.3 性能度量性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。 均方误差mean squared error MSE 在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数。 错误率error rate和精度accuracy 在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。 查准率（准确率precision）、查全率（召回率recall）、F1 错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下： FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negtive，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边： 正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。 “P-R曲线”正是描述查准/查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示： P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。 P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即： 特别地，当β=1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好 有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。 ROC与AUC 学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值(threshold)，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。 简单分析图像，可以得知：当FN=0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。 现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。 同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Uder ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。 2.5.4 代价敏感错误率与代价曲线 上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病—&gt;有疾病只是增多了检查，但有疾病—&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。 在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率为： 同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。 代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"西瓜书","slug":"西瓜书","permalink":"https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]},{"title":"西瓜书第1章","slug":"西瓜书第1章","date":"2021-08-04T05:04:25.000Z","updated":"2021-08-04T02:07:52.000Z","comments":true,"path":"2021/08/04/西瓜书第1章/","link":"","permalink":"https://allen123321.github.io/2021/08/04/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC1%E7%AB%A0/","excerpt":"","text":"第一章学习笔记1、引言傍晚小街路面上沁出微雨后的湿润，和熙的细风吹来，抬头看看天边的晚霞，嗯，明天又是一个好天气。走到水果摊旁，挑了个根蒂蜷缩、敲起来声音浊响的青绿西瓜，一边满心期待着皮薄肉厚瓢甜的爽落感，一边愉快地想着，这学期狠下了工夫，基础概念弄得清清楚楚，算法作业也是信手拈来，这门课成绩一定差不了！ 1.1、机器学习的定义正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。 另一本经典教材的作者Mitchell给出了一个形式化的定义，假设： P：计算机程序在某任务类T上的性能。 T：计算机程序希望实现的任务类。 E：表示经验，即历史的数据集。 若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。 1.2、机器学习的一些基本术语假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)，(色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义： 所有记录的集合为：数据集。 每一条记录为：一个实例（instance）或样本（sample）。 例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。 对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。 一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义： 所有训练样本的集合为：训练集（trainning set），[特殊]。 所有测试样本的集合为：测试集（test set），[一般]。 机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。 西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义： 预测值为离散值的问题为：分类（classification）。 预测值为连续值的问题为：回归（regression）。 我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义： 训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。 训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类（clustering）和关联规则。 训练training、验证validation/develop、测试testing 的作用学得模型后，使用其进行预测的过程称为测试。 为什么要将数据集划分为三个部分？三个部分的作用？三个部分数据集的比例应如何设定？ 另外一种常见的数据集划分方法是将数据集划分为两个部分（训练集和测试集），这种划分方法存在的问题在于，模型利用训练集数据进行训练，测试集数据进行模型泛化性测试。但我们不能利用测试集测试的bad case或者根据测试集的测试精度调整模型的参数。这是因为对于模型来说，数据集应该是只有训练集可见的，其他数据均不可见，如果利用测试集的结果调整模型相对于模型也”看到了“测试集的数据。将数据集划分为是独立同分布的三个部分就可以解决这个问题，将训练集数据用于模型训练，验证集（开发集）数据用于模型调参，测试集数据用于验证模型泛化性。 1.3 假设空间归纳induction和演绎deduction 归纳：从特殊到一般的“泛化”过程，从具体事实归结一般规律。 演绎：从一般到特殊的“特化”过程，从基础原理推演具体情况。 假设hypothesis space、版本空间hypothesis space 假设空间：监督学习（supervised learning）的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出 一个好的预测。模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间(hypothesis space)。我们也可以将学习过程看作一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集”匹配”的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确定，假设空间及其规模大小就确定了。 版本空间：与训练集一致的“假设集合”。 理解《机器学习》中的假设空间和版本空间 1.4、归纳偏好inductive bias在学习过程中对某种类型假设的偏好。有效的机器学习算法必有其归纳偏好。基于某种领域的知识而产生的归纳偏好，与特征选择不同，特征选择是基于训练样本分析选择有效的特征或者更加重视某类特征。 归纳偏好与问题本身匹配，偏好什么能让模型更好。 “奥卡姆剃刀”(Occam’s razor)是一种常用的、自然科学研究中最基本的原则，用于确立”正确的”偏好——即”若有多个假设与观察一致，则选最简单的那个“。 没有免费午餐定理No Free Lunch Theorem -NFL根据奥卡姆剃刀原则，下图(a)中A曲线的泛化能力要比曲线B强，但可能存在图(b)这样的情况，B曲线更好地满足测试集样本。也就是说，针对某种分布的样本空间（某种问题），算法A可能优于B，而针对另外一种分布的样本空间（另外一种问题），算法B可能优于A，这就引出了没有免费午餐定理。 没有免费午餐定理表明：若考虑所有潜在的问题，所有学习算法的误差期望相同，也就是所有算法都一样好。因此，脱离具体问题空谈什么学习算法更好没有意义，在某些问题上表现好的学习算法在另外一些问题上可能不尽人意。学习算法自身的归纳偏好与目标问题是否匹配往往起决定作用。","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"西瓜书","slug":"西瓜书","permalink":"https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]},{"title":"西瓜书第6章","slug":"ML-xgs-svm","date":"2021-07-29T07:06:02.000Z","updated":"2021-08-03T23:58:30.373Z","comments":true,"path":"2021/07/29/ML-xgs-svm/","link":"","permalink":"https://allen123321.github.io/2021/07/29/ML-xgs-svm/","excerpt":"","text":"6、支持向量机支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。 6.1 函数间隔与几何间隔对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示： 对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。 6.1.1 函数间隔在超平面w’x+b=0确定的情况下，|w’x+b|能够代表点x距离超平面的远近，易知：当w’x+b&gt;0时，表示x在超平面的一侧（正类，类标为1），而当w’x+b&lt;0时，则表示x在超平面的另外一侧（负类，类别为-1），因此（w’x+b）y 的正负性恰能表示数据点x是否被分类正确。于是便引出了*函数间隔的定义（functional margin）: 而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔： 可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念—几何间隔（geometrical margin）。 6.1.2 几何间隔几何间隔代表的则是数据点到超平面的真实距离，对于超平面w’x+b=0，w代表的是该超平面的法向量，设x为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x=x-r(w/||w||)，又x在超平面上，即w’x+b=0，代入即可得： 为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义： 从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。 6.2 最大间隔与支持向量通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为： 一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为： 对于y(w’x+b)=1的数据点，即下图中位于w’x+b=1或w’x+b=-1上的数据点，我们称之为支持向量（support vector），易知：对于所有的支持向量，它们恰好满足y(w’x+b)=1，而所有不是支持向量的点，有y(w’x+b)&gt;1。 6.3 从原始优化问题到对偶问题对于上述得到的目标函数，求1/||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为： 即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的对偶问题，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因： * 一是因为使用对偶问题更容易求解； * 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。 对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数： 上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于： 由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题： 这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。 （1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出： 将上述结果代入L得到： （2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。 （3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。 在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。 这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足： 6.4 核函数由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用映射的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为： 按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出： （1）原对偶问题变为： （2）原分类函数变为：​ 求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了核函数（Kernel）的概念。 因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效（低维计算，高维表现），从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为： （1）对偶问题： （2）分类函数： 因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件： 由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数： 6.5 软间隔支持向量机前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有噪声的情形，噪声数据（outlier）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。 为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了“软间隔”支持向量机的概念 * 允许某些数据点不满足约束y(w&#39;x+b)≥1； * 同时又使得不满足约束的样本尽可能少。 这样优化目标变为： 如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。 支持向量机中的损失函数为hinge损失，引入“松弛变量”，目标函数与约束条件可以写为： 其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到： 按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有： 将w代入L化简，便得到其对偶问题： 将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"}],"tags":[{"name":"西瓜书","slug":"西瓜书","permalink":"https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-07-29T01:09:06.613Z","updated":"2021-10-05T17:01:35.164Z","comments":true,"path":"2021/07/29/hello-world/","link":"","permalink":"https://allen123321.github.io/2021/07/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 发表文章新建文章 1$ hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; 发表草稿 1$ hexo p == hexo publish 生成静态文件 1$ hexo g == hexo generate 启动服务预览1$ hexo s == hexo server 部署到远程1$ hexo d == hexo deploy 清理缓存清除缓存文件 (db.json) 和已生成的静态文件 (public)1$ hexo clean 版本查看Hexo运行版本1$ hexo version 参考[1].Hexo博客搭建(1)——建站及部署","categories":[],"tags":[]}],"categories":[{"name":"OER","slug":"OER","permalink":"https://allen123321.github.io/categories/OER/"},{"name":"Tool","slug":"Tool","permalink":"https://allen123321.github.io/categories/Tool/"},{"name":"Machine learning","slug":"Machine-learning","permalink":"https://allen123321.github.io/categories/Machine-learning/"},{"name":"Deep learning","slug":"Deep-learning","permalink":"https://allen123321.github.io/categories/Deep-learning/"}],"tags":[{"name":"OpenEducationalResources","slug":"OpenEducationalResources","permalink":"https://allen123321.github.io/tags/OpenEducationalResources/"},{"name":"mysql","slug":"mysql","permalink":"https://allen123321.github.io/tags/mysql/"},{"name":"算法","slug":"算法","permalink":"https://allen123321.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"LaTex","slug":"LaTex","permalink":"https://allen123321.github.io/tags/LaTex/"},{"name":"PyQt5","slug":"PyQt5","permalink":"https://allen123321.github.io/tags/PyQt5/"},{"name":"李宏毅","slug":"李宏毅","permalink":"https://allen123321.github.io/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"西瓜书","slug":"西瓜书","permalink":"https://allen123321.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"}]}